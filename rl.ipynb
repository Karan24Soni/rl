{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6d87e7-0b60-43ee-b2ab-138b8880ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def bandit(A,k=10):\n",
    "    np.random.seed(42)\n",
    "    means=np.random.normal(0,1,k)\n",
    "    return np.random.normal(means[A],1)\n",
    "def epsilon_greedy_bandit(k,epsilon,steps):\n",
    "    Q=np.zeros(k)\n",
    "    N=np.zeros(k)\n",
    "    for _ in range(steps):\n",
    "        if np.random.random>epsilon:\n",
    "            Q_max=np.max(Q)\n",
    "            max_actions=np.where(Q==Q_max)[0]\n",
    "            A=np.random.choice(max_actions)\n",
    "        else:\n",
    "            A=np.random.randit(0,k)\n",
    "        R=bandit(A,k)\n",
    "        N[A] += 1\n",
    "        Q[A] += (R - Q[A]) / N[A]\n",
    "    return Q,N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdaef3a-652a-4578-b724-63945d57d545",
   "metadata": {},
   "source": [
    "page 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbfd11a-8e14-4017-ad7e-354560d05253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Value Function V(s):\n",
      "0.00 -5.26 -7.10 -7.62\n",
      "-5.26 -6.59 -7.16 -7.11\n",
      "-7.10 -7.16 -6.59 -5.27\n",
      "-7.62 -7.11 -5.27 0.00\n"
     ]
    }
   ],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self,rows=4,cols=4,gamma=0.9):\n",
    "        self.rows=rows\n",
    "        self.cols=cols\n",
    "        self.gamma=gamma\n",
    "        self.states=[(r,c) for r in range(rows) for c in range(cols)]\n",
    "        self.transitions={}\n",
    "        self.rewards={}\n",
    "        self.actions=['up','down','left','right']\n",
    "        self.terminal_states=[(0,0),(3,3)]\n",
    "        self._initialize_environment()\n",
    "    def _initialize_environment(self):\n",
    "        for state in self.states:\n",
    "            for action in self.actions:\n",
    "                next_state=self._get_next_state(state,action)\n",
    "                self.transitions[(state,action)]=[(next_state,1)]\n",
    "                if state in self.terminal_states:\n",
    "                    self.rewards[(state,action,next_state)]=0\n",
    "                else:\n",
    "                    self.rewards[(state,action,next_state)]=-1\n",
    "\n",
    "    def _get_next_state(self, state, action):\n",
    "        r, c = state\n",
    "        if state in self.terminal_states:\n",
    "            return state  # Stay in terminal state\n",
    "        if action == 'up':\n",
    "            return (max(r - 1, 0), c)\n",
    "        elif action == 'down':\n",
    "            return (min(r + 1, self.rows - 1), c)\n",
    "        elif action == 'left':\n",
    "            return (r, max(c - 1, 0))\n",
    "        elif action == 'right':\n",
    "            return (r, min(c + 1, self.cols - 1))\n",
    "        return state\n",
    "def iterative_policy_evaluation(gridworld,policy,theta=0.01):\n",
    "    V={state:0.0 for state in gridworld.states}\n",
    "    while True:\n",
    "        delta=0.0\n",
    "        for state in gridworld.states:\n",
    "            if state in gridworld.terminal_states:\n",
    "                continue\n",
    "            v=V[state]\n",
    "            new_value=0.0\n",
    "            for action in gridworld.actions:\n",
    "                pi_a_s=policy.get((state,action),0.0)\n",
    "                for next_state,prob in gridworld.transitions[(state,action)]:\n",
    "                    reward=gridworld.rewards.get((state,action,next_state),0.0)\n",
    "                    new_value += pi_a_s * prob * (reward + gridworld.gamma * V[next_state])\n",
    "            V[state] = new_value\n",
    "            delta = max(delta, abs(v - V[state]))  # Update max change\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    gridworld = GridWorld(rows=4, cols=4, gamma=0.9)\n",
    "\n",
    "    # Define a uniform policy: π(a|s) = 0.25 for each action\n",
    "    policy = {}\n",
    "    for state in gridworld.states:\n",
    "        for action in gridworld.actions:\n",
    "            policy[(state, action)] = 0.25  # Uniform probability\n",
    "\n",
    "    # Run iterative policy evaluation\n",
    "    V = iterative_policy_evaluation(gridworld, policy, theta=0.01)\n",
    "\n",
    "    # Print the value function\n",
    "    print(\"State-Value Function V(s):\")\n",
    "    for r in range(gridworld.rows):\n",
    "        row = []\n",
    "        for c in range(gridworld.cols):\n",
    "            state = (r, c)\n",
    "            row.append(f\"{V[state]:.2f}\")\n",
    "        print(\" \".join(row))\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "99a5ce5a-ea34-4fb0-8268-1431c570ef6a",
   "metadata": {},
   "source": [
    "4.3 policy iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc06e02f-0f34-4ea3-8144-63f21cecc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccec3819-adf2-4caf-8954-6abf2b715b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iterations(states,actions_per_state,transition_probs,gamma=0.9,theta=1e-4):\n",
    "    V={s:0.0 for s in states}\n",
    "    pi={s:np.random.choice(actions_per_state[s]) for s in states}\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta=0\n",
    "            for s in states:\n",
    "                v=V[s]\n",
    "                action=pi[s]\n",
    "                expected_value=0.0\n",
    "                for next_state,prob,reward in transition_probs.get((s,action),[]):\n",
    "                    expected_value+=prob*(reward+gamma*V[next_state])\n",
    "                V[s]=expected_value\n",
    "                delta=max(delta,abs(v-V[s]))\n",
    "                if delta>theta:\n",
    "                    break\n",
    "        policy_stable=True\n",
    "        for s in states:\n",
    "            old_action=pi[s]\n",
    "            best_action=None\n",
    "            # Find action that maximizes sum_{s',r} p(s',r|s,a) [r + gamma * V(s')]\n",
    "            max_value=float('-inf')\n",
    "            for action in actions_per_state[s]:\n",
    "                expected_value=0\n",
    "                for next_state,prob,reward in transition_probs.get((s,action),[]):\n",
    "                    expected_value+=prob*(reward+gamma*V[next_state])\n",
    "                if expected_value>max_value:\n",
    "                    max_value=expected_value\n",
    "                    best_action=action\n",
    "            pi[s]=best_action\n",
    "            if old_action != best_action:\n",
    "                policy_stable=False\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V,pi\n",
    "def create_example_mdp():\n",
    "    states = ['s0', 's1', 's2']\n",
    "    # Define state-dependent actions A(s)\n",
    "    actions_per_state = {\n",
    "        's0': ['a0', 'a1'],\n",
    "        's1': ['a0', 'a1'],\n",
    "        's2': ['a0', 'a1']\n",
    "    }\n",
    "    \n",
    "    # transition_probs[(s, a)] = [(next_state, prob, reward), ...]\n",
    "    transition_probs = {\n",
    "        ('s0', 'a0'): [('s1', 0.7, 1.0), ('s2', 0.3, 0.0)],\n",
    "        ('s0', 'a1'): [('s2', 1.0, 2.0)],\n",
    "        ('s1', 'a0'): [('s0', 0.5, 0.5), ('s1', 0.5, 1.0)],\n",
    "        ('s1', 'a1'): [('s2', 1.0, 1.5)],\n",
    "        ('s2', 'a0'): [('s0', 1.0, 0.0)],\n",
    "        ('s2', 'a1'): [('s1', 1.0, 0.5)]\n",
    "    }\n",
    "    \n",
    "    return states, actions_per_state, transition_probs\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9ed7ce7-d853-4061-8662-0bc94bd2ffbf",
   "metadata": {},
   "source": [
    "value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88443f4-acae-4859-a16d-96b13bc94042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self):\n",
    "        self.states=[]\n",
    "        self.actions=[]\n",
    "        self.transitions={}\n",
    "        self.gamma=0.0\n",
    "        self.theta=0.0\n",
    "def value_iteration(mdp):\n",
    "    V={s:0.0 for s in mdp.states}\n",
    "    for s in mdp.states:\n",
    "        if s=='terminal':\n",
    "            V[s]=0.0\n",
    "        \n",
    "    while True:\n",
    "        delta=0.0\n",
    "        for s in mdp.states:\n",
    "            if s=='terminal':\n",
    "                continue\n",
    "            old_v=V[s]\n",
    "            new_v=float('-inf')\n",
    "            for a in mdp.actions:\n",
    "                expected_value=0\n",
    "                for next_state,reward,prob in mdp.transtions[s][a]:\n",
    "                    expected_value+=prob*(reward+gamma*V[new_state])\n",
    "                new_v=max(new_v,expected_value)\n",
    "            V[s]=new_v\n",
    "            delta=max(delta,abs(old_v-[s]))\n",
    "            if delta<mdp.theta:\n",
    "                break\n",
    "    policy = {}\n",
    "    for s in mdp.states:\n",
    "        if s == 'terminal':\n",
    "            policy[s] = None\n",
    "            continue\n",
    "        best_action, best_value = None, float('-inf')\n",
    "        for a in mdp.actions:\n",
    "            expected_value = 0\n",
    "            for next_state, reward, prob in mdp.transitions[s][a]:\n",
    "                expected_value += prob * (reward + mdp.gamma * V[next_state])\n",
    "            if expected_value > best_value:\n",
    "                best_value = expected_value\n",
    "                best_action = a\n",
    "        policy[s] = best_action\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4be740d2-ff8f-45af-9132-c018a313c75b",
   "metadata": {},
   "source": [
    "monte carlo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e071d956-62f6-4c33-b3f1-1684abdb7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97706899-a105-44dd-9fa0-f98f0ae1147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_evaluation(policy,env,num_episodes=1000,gamma=1.0):\n",
    "    V=defaultdict(float)\n",
    "    Returns=defaultdict(float)\n",
    "    for episode in range(num_episodes):\n",
    "        episode_states=[]\n",
    "        episode_actions=[]\n",
    "        episode_rewards=[]\n",
    "        state=env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            action_probs=policy(state)\n",
    "            action=np.random.choice(len(action_probs),p=action_probs)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state=next_state\n",
    "        G = 0\n",
    "        for t in range(len(episode_states) - 1, -1, -1):\n",
    "            G = episode_rewards[t] + gamma * G\n",
    "            state = episode_states[t]\n",
    "            \n",
    "            # First-visit MC: only update if state hasn't appeared earlier in episode\n",
    "            if state not in episode_states[:t]:\n",
    "                Returns[state].append(G)\n",
    "                V[state] = np.mean(Returns[state])\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c029ec2-d255-4ed6-9bfe-e3a18f0f72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(env,num_episodes=1000,gamma=1.0):\n",
    "    policy=defaultdict(lambda:0)\n",
    "    Q=defaultdict(lambda:defaultdict(float))\n",
    "    Returns=defaultdict(lambda:defaultdict(list))\n",
    "    states=range(env.observation_space.n)\n",
    "    actions=range(env.action_space.n)\n",
    "    for episode in range(num_episodes):\n",
    "        state=np.random.choice(states)\n",
    "        action=np.random.choice(actions)\n",
    "        env.reset()\n",
    "        env.state=state\n",
    "        episode_states=[state]\n",
    "        episode_actions=[action]\n",
    "        episode_rewards=[]\n",
    "\n",
    "        next_step,reward,done,_=env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        state=next_state\n",
    "        while not done:\n",
    "            action=policy[state]\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state=next_state\n",
    "        G=0\n",
    "        for t in range(len(episode_states)-1,-1,-1):\n",
    "            state,action=episode_states[t],episode_actions[t]\n",
    "            G=episode_rewards[t]+gamma*G\n",
    "            earlier_pairs=set(zip(episode_states[:t],episode_actions[:t]))\n",
    "            if (state,action) not in earlier_pairs:\n",
    "                Returns[state][action].append(G)\n",
    "                Q[state][action]=np.mean(Returns[state][action])\n",
    "                policy[state] = np.argmax([Q[state][a] for a in actions])\n",
    "    \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63160968-88e3-4b5e-a4e0-38d0530464b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es_efficient(policy,num_episodes=10000,gamma=1.0):\n",
    "    policy=defaultdict(lambda:0)\n",
    "    Q=defaultdict(lambda:defaultdict(float))\n",
    "    Count=defaultdict(lambda:defaultdict(int))\n",
    "    states=range(env.observation_space.n)\n",
    "    actions=range(env.action_space.n)\n",
    "    for episode in range(num_episodes):\n",
    "        state=np.random.choice(states)\n",
    "        action=np.random.choice(actions)\n",
    "        env.reset()\n",
    "        env.state=state\n",
    "        episode_states=[state]\n",
    "        episode_actions=[action]\n",
    "        episode_rewards=[]\n",
    "        next_state,reward,done,_=env.step(action)\n",
    "        episode_rewards.append(reward)\n",
    "        state=next_state\n",
    "        while not done:\n",
    "            action=policy(state)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state=next_state\n",
    "        G=0\n",
    "        for t in range(len(episode_states)-1,-1,-1):\n",
    "            state,action=episode_states[t],episode_actions[t]\n",
    "            G=episode_rewards[t]+gamma*G\n",
    "            earlier_pairs = set(zip(episode_states[:t], episode_actions[:t]))\n",
    "            if (state, action) not in earlier_pairs:\n",
    "                Count[state][action] += 1\n",
    "                Q[state][action] += (1 / Count[state][action]) * (G - Q[state][action])\n",
    "                \n",
    "                # Policy improvement\n",
    "                policy[state] = np.argmax([Q[state][a] for a in actions])\n",
    "    \n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "raw",
   "id": "229f8525-040b-4ad9-bb0d-913b73ebc150",
   "metadata": {},
   "source": [
    "Step-by-Step Example\n",
    "Let’s simulate one episode:\n",
    "\n",
    "Setup:\n",
    "\n",
    "Randomly choose initial state: state = 5 (grid position [1, 1]).\n",
    "Randomly choose initial action: action = 2 (right).\n",
    "Initialize lists: episode_states = [5], episode_actions = [2], episode_rewards = [].\n",
    "\n",
    "\n",
    "Initial Step (Outside Loop):\n",
    "\n",
    "Set environment to state 5: env.state = 5.\n",
    "Take action 2 (right): next_state, reward, done, _ = env.step(2).\n",
    "\n",
    "In FrozenLake, moving right from state 5 (if not slippery) goes to state 6 ([1, 2]).\n",
    "Reward is 0 (not at goal yet).\n",
    "Done is False (state 6 is not terminal).\n",
    "\n",
    "\n",
    "Append reward: episode_rewards = [0].\n",
    "Update state: state = 6.\n",
    "Lists now: episode_states = [5], episode_actions = [2], episode_rewards = [0].\n",
    "\n",
    "\n",
    "Subsequent Steps (Inside While Loop):\n",
    "\n",
    "Check done: False, so enter loop.\n",
    "First iteration:\n",
    "\n",
    "Current state is 6. Policy chooses action: action = policy[6], say 1 (down).\n",
    "Take action 1: next_state, reward, done, _ = env.step(1).\n",
    "\n",
    "Down from state 6 goes to state 10 ([2, 2]).\n",
    "Reward is 0 (not at goal).\n",
    "Done is False (state 10 is not terminal).\n",
    "\n",
    "\n",
    "Append: episode_states = [5, 6], episode_actions = [2, 1], episode_rewards = [0, 0].\n",
    "Update state: state = 10.\n",
    "\n",
    "\n",
    "Second iteration:\n",
    "\n",
    "State is 10. Policy chooses action: action = policy[10], say 2 (right).\n",
    "Take action 2: next_state, reward, done, _ = env.step(2).\n",
    "\n",
    "Right from state 10 goes to state 11 ([2, 3]).\n",
    "Reward is 0.\n",
    "Done is False.\n",
    "\n",
    "\n",
    "Append: episode_states = [5, 6, 10], episode_actions = [2, 1, 2], episode_rewards = [0, 0, 0].\n",
    "Update state: state = 11.\n",
    "\n",
    "\n",
    "Third iteration:\n",
    "\n",
    "State is 11. Policy chooses action: action = policy[11], say 1 (down).\n",
    "Take action 1: next_state, reward, done, _ = env.step(1).\n",
    "\n",
    "Down from state 11 goes to state 15 ([3, 3], the goal).\n",
    "Reward is 1 (reached goal).\n",
    "Done is True (goal is terminal).\n",
    "\n",
    "\n",
    "Append: episode_states = [5, 6, 10, 11], episode_actions = [2, 1, 2, 1], episode_rewards = [0, 0, 0, 1].\n",
    "Update state: state = 15.\n",
    "\n",
    "\n",
    "Loop ends: done is True, exit loop.\n",
    "\n",
    "\n",
    "Result:\n",
    "\n",
    "Episode trajectory: <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub><mo>=</mo><mn>5</mn><mo separator=\"true\">,</mo><msub><mi>A</mi><mn>0</mn></msub><mo>=</mo><mn>2</mn><mo separator=\"true\">,</mo><msub><mi>R</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>S</mi><mn>1</mn></msub><mo>=</mo><mn>6</mn><mo separator=\"true\">,</mo><msub><mi>A</mi><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><msub><mi>R</mi><mn>2</mn></msub><mo>=</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>=</mo><mn>10</mn><mo separator=\"true\">,</mo><msub><mi>A</mi><mn>2</mn></msub><mo>=</mo><mn>2</mn><mo separator=\"true\">,</mo><msub><mi>R</mi><mn>3</mn></msub><mo>=</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>S</mi><mn>3</mn></msub><mo>=</mo><mn>11</mn><mo separator=\"true\">,</mo><msub><mi>A</mi><mn>3</mn></msub><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><msub><mi>R</mi><mn>4</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\"> S_0=5, A_0=2, R_1=0, S_1=6, A_1=1, R_2=0, S_2=10, A_2=2, R_3=0, S_3=11, A_3=1, R_4=1 </annotation></semantics></math>.\n",
    "Lists:\n",
    "\n",
    "episode_states = [5, 6, 10, 11]\n",
    "episode_actions = [2, 1, 2, 1]\n",
    "episode_rewards = [0, 0, 0, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Why Two Appends?\n",
    "\n",
    "First append: Captures the reward (0) from the random action (2: right) in state 5. This is outside the loop because it’s the special “exploring starts” step.\n",
    "Second append (in loop): Captures rewards (0, 0, 1) from policy actions (1: down, 2: right, 1: down) in states 6, 10, and 11. The loop handles all steps after the initial random action.\n",
    "\n",
    "Without the first append, we’d miss the reward from the random action. Without the loop’s append, we’d miss rewards from the policy’s actions.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "869201ee-89b0-4ebf-8639-8321df9a6bb3",
   "metadata": {},
   "source": [
    " Monte Carlo policy iteration with epsilon-soft policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment with step, reset, observation_space, action_space\n",
    "        num_episodes: Number of episodes to run\n",
    "        epsilon: Exploration parameter (epsilon > 0)\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        policy: Epsilon-soft policy (state -> action probabilities)\n",
    "        Q: Action-value function\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c228c175-9505-4127-831b-ff9a730cd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_soft_monte_carlo(env,episodes=1000,epsilon=0.1,gamma=1.0):\n",
    "    num_actions=env.action_space.n\n",
    "    policy=defaultdict(lambda:np.ones(num_actions)/num_actions)\n",
    "    Q=defaultdict(lambda:defaultdict(float))\n",
    "    Returns=defaultdict(lambda:defaultdict(list))\n",
    "    for episode in range(episodes):\n",
    "        state=env.reset()\n",
    "        episode_states=[]\n",
    "        episode_actions=[]\n",
    "        episode_rewards=[]\n",
    "        done=False\n",
    "        while not done:\n",
    "            action_prob=policy[state]\n",
    "            action=np.random.choice(num_actions,p=action_prob)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state=next_state\n",
    "        G=0\n",
    "        for t in range(len(episode_states)-1,-1,-1):\n",
    "            state,action=episode_states[t],episode_actions[t]\n",
    "            G=episode_rewards+gamma*G\n",
    "            earlier_pairs=set(zip(episode_states[:t],episode_actions[:t]))\n",
    "            if (state, action) not in earlier_pairs:\n",
    "                Returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(Returns[state][action])\n",
    "                \n",
    "                # Update epsilon-soft policy\n",
    "                best_action = np.argmax([Q[state][a] for a in range(num_actions)])\n",
    "                for a in range(num_actions):\n",
    "                    if a == best_action:\n",
    "                        policy[state][a] = 1 - epsilon + epsilon / num_actions\n",
    "                    else:\n",
    "                        policy[state][a] = epsilon / num_actions\n",
    "    \n",
    "    return policy, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bcbce82-ab49-4c8d-a121-edf03954c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo(env,num_episodes=1000,epsilon=0.1,gamma=1.0):\n",
    "    num_actions=env.action_space.n\n",
    "    Q=defaultdict(lambda:defaultdict(float))\n",
    "    C=defaultdict(lambda:defaultdict(float))\n",
    "    pi=defaultdict(lambda:0)\n",
    "\n",
    "    def behavior_policy(state):\n",
    "        probs=np.ones(num_actions)*epsilon/num_actions\n",
    "        best_actions=pi[state]\n",
    "        probs[best_action]+=(1-epsilon)\n",
    "        return probs\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset()\n",
    "        episode_states=[]\n",
    "        episode_actions=[]\n",
    "        episode_rewards=[]\n",
    "        done=False\n",
    "\n",
    "        while not done:\n",
    "            action_probs=behavior_policy(state)\n",
    "            action=np.random.choice(num_actions,p=action_probs)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state=next_state\n",
    "        G=0\n",
    "        W=1\n",
    "        for t in range(len(episode_states)-1,-1,-1):\n",
    "            state,action=episode_states[t],episode_actions[t]\n",
    "            G=episode_rewards[t]+gamma*G\n",
    "            if W==0:\n",
    "                break\n",
    "            C[state][action]+=W\n",
    "            Q[state][action]+=(W/C[state][action])*(G-Q[state][action])\n",
    "            pi[state]=np.argmax([Q[state][a] for a in range(num_actions)])\n",
    "            pi_prob = 1.0 if action == pi[state] else 0.0  # Deterministic pi\n",
    "            b_prob = behavior_policy(state)[action]\n",
    "            W *= pi_prob / b_prob\n",
    "            \n",
    "    return pi, Q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d476a13f-1901-43f7-a8ca-c2f19a18b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo_early_exit(env,num_episodes=1000,epsilon=0.1,gamma=1):\n",
    "    num_actions=env.action_space.n\n",
    "    Q=defaultdict(lambda:defaultdict(float))\n",
    "    C=defaultdict(lambda:defaultdict(float))\n",
    "    pi=defaultdict(lambda:0)\n",
    "    def behavior_policy(state):\n",
    "        probs=np.ones(num_actions)*epsilon/num_actions\n",
    "        probs[pi[state]]+=(1-epsilon)\n",
    "        return probs\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset()\n",
    "        episode_states=[]\n",
    "        episode_actions=[]\n",
    "        episode_rewards=[]\n",
    "        done=False\n",
    "        while not done:\n",
    "            action_prob=behavior_policy(state)\n",
    "            action=np.random.choice(num_actions,p=action_prob)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state=next_state\n",
    "        G=0\n",
    "        W=1\n",
    "        for t in range(len(episode_states)-1,-1,-1):\n",
    "            state,action== episode_states[t], episode_actions[t]\n",
    "            G = episode_rewards[t] + gamma * G\n",
    "            \n",
    "            # Update C and Q\n",
    "            C[state][action] += W\n",
    "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "            \n",
    "            # Update target policy (argmax with tie-breaking)\n",
    "            q_values = [Q[state][a] for a in range(num_actions)]\n",
    "            pi[state] = np.argmax(q_values)  # Lowest index breaks ties\n",
    "            \n",
    "            # Early exit if action doesn't match target policy\n",
    "            if action != pi[state]:\n",
    "                break\n",
    "            \n",
    "            # Update importance sampling weight\n",
    "            b_prob = behavior_policy(state)[action]\n",
    "            W *= 1 / b_prob\n",
    "    \n",
    "    return pi, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d2037-c618-4bb4-85b3-cec055d537a3",
   "metadata": {},
   "source": [
    "# Blackjack #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "335c49b9-5efb-47ee-bb47-b7907f3ae96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a39af12a-6a5a-4c45-91f3-048eb0eaaf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invert_map(lst):\n",
    "    return {v:i for i,v in enumerate(lst)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10f1d1-c0e3-4b75-8b61-092ef4e72fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJack:\n",
    " \n",
    "    cards=['A',2,3,4,5,6,7,8,9,10,10,10,10]\n",
    "    agent_sums=[12,13,14,15,16,17,18,19,20,21]\n",
    "    dealers_cards=['A',2,3,4,5,6,7,8,9,10]\n",
    "    ace_usability = [False, True]  # Referenced as 0, 1\n",
    "    actions_possible = [\"S\", \"H\"]  # Referenced as 0, \n",
    "\n",
    "    agent_sums_map,dealers_cards_map,ace_usability_map,actions_possible_map,cards_map=map(\n",
    "        create_invert_map,[agent_sums,dealers_cards,ace_usability,actions_possible,cards])\n",
    "    def __init__(self,M,epsilon,alpha,seed=0):\n",
    "        self.seed=seed\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        self.Q_hist=None\n",
    "        self.initialize()\n",
    "    def initialize(self):\n",
    "        self.Q=np.zeros((len(self.agent_sums),len(self.dealers_cards),len(self.ace_usability),len(self.action_possible)))\n",
    "        self.C=np.zeros_like(self.Q,dtype=int)\n",
    "        # Create the data directory if it doesn't already exist.\n",
    "        if not os.path.exists('data'):\n",
    "            os.mkdir('data')\n",
    "    def sampled_cards(self,N):\n",
    "        sampled_cards=np.random.choice(self.cards,size=N,replace=True)\n",
    "        return [c if c=='A' else int(c) for c in sampled_cards]\n",
    "    def map_to_indices(self,agent_sum,dealers_card,usable_ace):\n",
    "        return (\n",
    "            self.agent_sums_map[agent_sum],\n",
    "            self.dealers_cards_map[dealer_card],\n",
    "            self.ace_usability_map[usable_ace])\n",
    "    def behavior_policy(self, agent_sum, dealers_card, useable_ace):\n",
    "        \"\"\"Returns H (Hit) or S (Stick) to determine the actions to take during the game.\"\"\"\n",
    "        agent_sum_idx, dealers_card_idx, useable_ace_idx = self.map_to_indices(\n",
    "            agent_sum, dealers_card, useable_ace\n",
    "        )\n",
    "        greedy_action = self.Q[\n",
    "            agent_sum_idx, dealers_card_idx, useable_ace_idx, :\n",
    "        ].argmax()\n",
    "        do_greedy = np.random.binomial(1, 1 - self.epsilon + (self.epsilon / 2))\n",
    "        if do_greedy:\n",
    "            return self.actions_possible[int(greedy_action)]\n",
    "        else:\n",
    "            return self.actions_possible[int(not greedy_action)]\n",
    "\n",
    "    def target_policy(self, agent_sum, dealers_card, useable_ace):\n",
    "        \"\"\"\n",
    "        The target policy is the same as the behavior policy in on-policy learning. Note: this method is never actually\n",
    "        referenced.\n",
    "        \"\"\"\n",
    "        return self.behavior_policy(\n",
    "            agent_sum=agent_sum, dealers_card=dealers_card, useable_ace=useable_ace\n",
    "        )\n",
    "\n",
    "    def is_ratio(self, states_remaining, actions_remaining):\n",
    "        \"\"\"The Importance Sampling ratio that can be overwritten for off policy control.\"\"\"\n",
    "        return 1\n",
    "    @staticmethod\n",
    "    def calc_sum_useable_ace(cards):\n",
    "        cards_sum=sum([c for c in cards if c!='A'])\n",
    "        ace_count=sum([c for c in cards if c=='A'])\n",
    "\n",
    "        if ace_count==0:\n",
    "            return cards_sum,False\n",
    "        else:\n",
    "            cards_sum_0=cards_sum+ace_sum\n",
    "            cards_sum_1=cards_sum+10+ace_sum\n",
    "            if cards_sum_1>21:\n",
    "                return cards_sum_0,False\n",
    "            return cards_sum_1,True\n",
    "    def play_game(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            states: a list of states, one for each time state.\n",
    "            actions: a list of agent actions, one for each time state.\n",
    "            reward: final return of the game. Either -1, 0, 1\n",
    "            how: string describing how the game ended.\n",
    "        \"\"\"\n",
    "\n",
    "        agent_cards = self.sample_cards(2)  # Agent is delt 2 cards\n",
    "        dealers_card = self.sample_cards(1)[0]  # Dealer is (effectively) delt 1 cards\n",
    "\n",
    "        states = [[agent_cards, dealers_card]]\n",
    "        actions = []\n",
    "        # Hit until agent_sum >= 12\n",
    "        while True:\n",
    "            agent_cards,dealer_card=states[-1]\n",
    "            agent_sum,_=self.calc_sum_useable_ace(agent_cards)\n",
    "            if agent_sum<12:\n",
    "                actions.append('H')\n",
    "                agent_cards_next=agent_cards+self.sample_cards(1)\n",
    "                states.append([agent_cards_next,dealer_card])\n",
    "            else:\n",
    "                break\n",
    "         # Play game according to agents policy\n",
    "        while True:\n",
    "            agent_cards,dealers_card=states[-1]\n",
    "            agent_sum,useable_ace=self.calc_sum_useable_ace(agent_cards)\n",
    "            action=self.behavior_policy(agent_sum=agent_sum,useable_ace=useable_ace,dealers_card=dealers_card)\n",
    "            actions.append(action)\n",
    "            if action=='S':\n",
    "                states.append([action_cards,dealers_card])\n",
    "                break\n",
    "            else:\n",
    "                agent_cards_next=agent_card+self.sample_state(1)\n",
    "                states.append([agent_cards_next,dealers_card])\n",
    "                agent_sum_next,useable_ace=self.calc_sum_useable_ace(agent_cards_next)\n",
    "                if agent_cards_next>21:\n",
    "                    how='bust'\n",
    "                    reward=-1\n",
    "                    return states,actions,reward,how\n",
    "        # Dealer plays\n",
    "        dealers_cards = [states[-1][1]] + self.sample_cards(1)  # Turn over card\n",
    "        while True:\n",
    "            dealers_sum, _ = self.calc_sum_useable_ace(dealers_cards)\n",
    "            if dealers_sum > 21:\n",
    "                how = \"dealer_bust:\" + \",\".join([str(c) for c in dealers_cards])\n",
    "                reward = 1\n",
    "                return states, actions, reward, how\n",
    "            if dealers_sum > 16:\n",
    "                break\n",
    "            else:\n",
    "                dealers_cards += self.sample_cards(1)\n",
    "\n",
    "        agent_sum, useable_ace = self.calc_sum_useable_ace(states[-1][0])\n",
    "\n",
    "        # If the game hasn't already ended, determine the winner.\n",
    "        if agent_sum == dealers_sum:\n",
    "            return states, actions, 0, f\"{agent_sum} = {dealers_sum}\"\n",
    "        elif agent_sum > dealers_sum:\n",
    "            return states, actions, 1, f\"{agent_sum} > {dealers_sum}\"\n",
    "        else:\n",
    "            return states, actions, -1, f\"{agent_sum} < {dealers_sum}\"\n",
    "\n",
    "\n",
    "    def get_hyper_str(self):\n",
    "        \"\"\"Returns a string uniquely identifying the class arguments.\"\"\"\n",
    "        return f\"M{self.M}__epsilon{str(self.epsilon).replace('.', '_')}__alpha{str(self.alpha).replace('.', '_')}__seed{str(self.seed)}\"\n",
    "    def get_file_names(self):\n",
    "        hyper_str = self.get_hyper_str()\n",
    "        Q_name = \"data\\\\Q_\" + hyper_str\n",
    "        C_name = \"data\\\\C_\" + hyper_str\n",
    "        Q_hist_name = \"data\\\\Q_hist_\" + hyper_str\n",
    "        return Q_name, C_name, Q_hist_name\n",
    "    def save(self):\n",
    "        Q_name, C_name, Q_hist_name = self.get_file_names()\n",
    "        print(f\"Saving {Q_name}\")\n",
    "        np.save(Q_name, self.Q)\n",
    "        print(f\"Saving {C_name}\")\n",
    "        np.save(C_name, self.C)\n",
    "        if self.Q_hist is not None:\n",
    "            print(f\"Saving {Q_hist_name}\")\n",
    "            self.Q_hist.to_pickle(Q_hist_name + \".pkl\")\n",
    "\n",
    "    def load(self):\n",
    "        Q_name, C_name, Q_hist_name = self.get_file_names()\n",
    "        print(f\"Loading...\")\n",
    "        self.Q = np.load(Q_name + \".npy\")\n",
    "        self.C = np.load(C_name + \".npy\")\n",
    "        try:\n",
    "            self.Q_hist = pd.read_pickle(Q_hist_name + \".pkl\")\n",
    "        except:\n",
    "            print(\"No Q hist to load\")\n",
    "    def _get_ms(self):\n",
    "        \"\"\"Returns a list of episodes index's (sometimes called 'm's) for which we record action-value pairs.\"\"\"\n",
    "        return list(range(0, self.M + 1, 1000))\n",
    "    def mc_control(self, track_history=True):\n",
    "\n",
    "        \"\"\"Return the algorithm to learn the Q-table.\"\"\"\n",
    "\n",
    "        self.initialize()\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        Q_hist = []\n",
    "        ms = self._get_ms()\n",
    "\n",
    "        for m in tqdm(range(self.M + 1)):\n",
    "            states, actions, reward, how = self.play_game()\n",
    "\n",
    "            if track_history:\n",
    "                if m in ms:\n",
    "                    Q_hist.append(self.get_df(\"Q\").assign(m=m))\n",
    "\n",
    "            for i, (state, action) in enumerate(zip(states[:-1], actions)):\n",
    "\n",
    "                agent_cards, dealers_card = state\n",
    "                agent_sum, useable_ace = self.calc_sum_useable_ace(agent_cards)\n",
    "\n",
    "                if agent_sum < 12:\n",
    "                    continue\n",
    "\n",
    "                agent_sum_idx, dealers_card_idx, useable_ace_idx = self.map_to_indices(\n",
    "                    agent_sum, dealers_card, useable_ace\n",
    "                )\n",
    "                actions_idx = self.actions_map[action]\n",
    "\n",
    "                q_val = self.Q[\n",
    "                    agent_sum_idx, dealers_card_idx, useable_ace_idx, actions_idx\n",
    "                ]\n",
    "\n",
    "                rho = self.is_ratio(states[i + 1 :], actions[i + 1 :])\n",
    "\n",
    "                self.Q[\n",
    "                    agent_sum_idx, dealers_card_idx, useable_ace_idx, actions_idx\n",
    "                ] = q_val + self.alpha * (rho * reward - q_val)\n",
    "\n",
    "                self.C[\n",
    "                    agent_sum_idx, dealers_card_idx, useable_ace_idx, actions_idx\n",
    "                ] += 1\n",
    "\n",
    "        if track_history:\n",
    "            self.Q_hist = pd.concat(Q_hist, axis=0)\n",
    "\n",
    "        self.save()\n",
    "    def plot_over_m(\n",
    "        self, agent_sum, dealers_card, useable_ace, width=400, height=200, **kwargs\n",
    "    ):\n",
    "        \"\"\"Creates a line plot of the action-value of hit and stick as episodes are processed. Note: we don't record\n",
    "        action-value after every episodes, so not all action-values are shown.\"\"\"\n",
    "\n",
    "        if dealers_card == \"A\":\n",
    "            dealers_card = f\"'{dealers_card}'\"\n",
    "\n",
    "        df = self.Q_hist.query(\n",
    "            f\"(agent_sum == {agent_sum}) & (dealers_card == {dealers_card}) & (useable_ace == {useable_ace})\"\n",
    "        )[[\"Q\", \"action\", \"m\"]]\n",
    "\n",
    "        return (\n",
    "            alt.Chart(df)\n",
    "            .mark_line(**kwargs)\n",
    "            .encode(x=\"m\", y=\"Q\", color=\"action\")\n",
    "            .properties(height=height, width=width)\n",
    "        )\n",
    "\n",
    "    def make_slice_df(self, Q_or_C_slice, name):\n",
    "        df = pd.DataFrame(\n",
    "            Q_or_C_slice, index=self.agent_sums, columns=self.dealers_cards\n",
    "        )\n",
    "        df.index.name = \"agent_sum\"\n",
    "        df.columns.name = \"dealers_card\"\n",
    "        return df.stack().to_frame(name)\n",
    "    def get_df(self, which=\"Q\"):\n",
    "\n",
    "        assert which in [\"Q\", \"C\"]\n",
    "\n",
    "        array = getattr(self, which)\n",
    "\n",
    "        df = []\n",
    "        for idx1 in [0, 1]:\n",
    "            for idx2 in [0, 1]:\n",
    "\n",
    "                slc = array[:, :, idx1, idx2]\n",
    "                slc_df = self.make_slice_df(slc, which)\n",
    "\n",
    "                if which == \"Q\":\n",
    "                    opt = array.argmax(-1)[:, :, idx1]\n",
    "                    is_opt = opt == idx2\n",
    "\n",
    "                    slc_df = pd.concat(\n",
    "                        [slc_df, self.make_slice_df(is_opt.astype(int), \"opt\")], axis=1\n",
    "                    )\n",
    "\n",
    "                slc_df = slc_df.reset_index().assign(\n",
    "                    useable_ace=self.ace_usability[idx1],\n",
    "                    action=self.actions_possible[idx2],\n",
    "                )\n",
    "                df.append(slc_df)\n",
    "\n",
    "        return pd.concat(df, axis=0)\n",
    "    def plot(self, which=\"Q\", height=200, width=350):\n",
    "        \"\"\"Returns 2-by-2 grid of heatmaps giving either the action values or the counts each state-action pair was visited.\"\"\"\n",
    "\n",
    "        assert which in [\"Q\", \"C\"]\n",
    "\n",
    "        df = self.get_df(which)\n",
    "\n",
    "        if \"Q\" in df.columns:\n",
    "            col = \"Q\"\n",
    "            color = alt.Color(col, scale=alt.Scale(domain=[-1, 1]))\n",
    "        else:\n",
    "            col = \"C\"\n",
    "            color = alt.Color(col)\n",
    "\n",
    "        heatmap = (\n",
    "            alt.Chart(df)\n",
    "            .mark_rect()\n",
    "            .encode(x=\"agent_sum:O\", y=\"dealers_card:O\", color=color,)\n",
    "        )\n",
    "        df[\"rounded\"] = df[col].round(2)\n",
    "\n",
    "        args = dict(x=\"agent_sum:O\", y=\"dealers_card:O\", text=\"rounded\")\n",
    "        txt = alt.Chart(df).mark_text().encode(**args)\n",
    "\n",
    "        if which == \"Q\":\n",
    "            opt = (\n",
    "                alt.Chart(df)\n",
    "                .mark_rect(fill=None, strokeWidth=2, stroke=\"green\")\n",
    "                .encode(x=\"agent_sum:O\", y=\"dealers_card:O\")\n",
    "                .transform_filter(alt.datum.opt == 1)\n",
    "            )\n",
    "            chart = heatmap + opt + txt\n",
    "        else:\n",
    "            chart = heatmap + txt\n",
    "\n",
    "        return chart.properties(height=height, width=width).facet(\n",
    "            row=\"action\", column=\"useable_ace\"\n",
    "        )\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "                       \n",
    "            \n",
    "                \n",
    "                    \n",
    "\n",
    "\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e514c-9028-4bd9-8621-856ec6cfd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackJackOffPolicy(BlackJack):\n",
    "    def __init__(self,M,alpha,seed=0):\n",
    "        self.M=M\n",
    "        self.alpha=alpha\n",
    "        self.Q_hist=None\n",
    "        self.seed=seed\n",
    "        self.initialize()\n",
    "    def behavior_policy(self, agent_sum, dealers_card, useable_ace):\n",
    "        \"\"\"Behavior policy is to select hit or stick with a 50/50 probability.\"\"\"\n",
    "        return np.random.choice(self.actions_possible)\n",
    "\n",
    "    def target_policy(self, agent_sum, dealers_card, useable_ace):\n",
    "        agent_sum_idx, dealers_card_idx, useable_ace_idx = self.map_to_indices(\n",
    "            agent_sum, dealers_card, useable_ace\n",
    "        )\n",
    "        greedy_action = self.Q[\n",
    "            agent_sum_idx, dealers_card_idx, useable_ace_idx, :\n",
    "        ].argmax()\n",
    "        return self.actions_possible[greedy_action]\n",
    "\n",
    "    def get_hyper_str(self):\n",
    "        return f\"M{self.M}__OP__alpha{str(self.alpha).replace('.', '_')}__seed{str(self.seed)}\"\n",
    "\n",
    "    def is_ratio(self, states_remaining, actions_remaining):\n",
    "        \"\"\"\n",
    "        In this case, the target policy is deterministic, so the importance sampling ratio will only be nonzero when the\n",
    "        deterministic policy would have target that action.\n",
    "\n",
    "        \"\"\"\n",
    "        for state, action in zip(states_remaining, actions_remaining):\n",
    "            agent_cards, dealers_card = state\n",
    "            agent_sum, useable_ace = self.calc_sum_useable_ace(agent_cards)\n",
    "            target_action = self.target_policy(agent_sum, dealers_card, useable_ace)\n",
    "            if target_action != action:\n",
    "                return 0.0\n",
    "\n",
    "        return 1.0 / (0.5 ** len(actions_remaining))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db820c42-7859-4db3-a64c-43d480eaa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.state_space = list(range(4))  # States: 0, 1, 2, 3\n",
    "        self.action_space = list(range(4))  # Actions: 0=left, 1=right, 2=up, 3=down\n",
    "        self.goal = 3  # Goal at bottom-right\n",
    "        self.trap = 1  # Trap at top-right\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Initialize to starting state (top-left).\"\"\"\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action, return (next_state, reward, done, info).\"\"\"\n",
    "        # Current position (row, col) from state (0 to 3)\n",
    "        row, col = divmod(self.state, 2)  # 2x2 grid\n",
    "        \n",
    "        # Compute next position based on action\n",
    "        if action == 0:  # Left\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == 1:  # Right\n",
    "            col = min(col + 1, 1)\n",
    "        elif action == 2:  # Up\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 3:  # Down\n",
    "            row = min(row + 1, 1)\n",
    "        \n",
    "        # New state\n",
    "        next_state = row * 2 + col\n",
    "        \n",
    "        # Reward and done\n",
    "        reward = -0.1  # Step penalty\n",
    "        done = False\n",
    "        if next_state == self.goal:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        elif next_state == self.trap:\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "        \n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def render(self):  # Optional: Visualize state\n",
    "        grid = [['.' for _ in range(2)] for _ in range(2)]\n",
    "        row, col = divmod(self.state, 2)\n",
    "        grid[row][col] = 'A'  # Agent\n",
    "        grid[0][1] = 'T'  # Trap\n",
    "        grid[1][1] = 'G'  # Goal\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "\n",
    "# Test with Q-learning\n",
    "def q_learning(env, num_episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    from collections import defaultdict\n",
    "    Q = defaultdict(lambda: defaultdict(float))\n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(num_actions)\n",
    "            else:\n",
    "                action = np.argmax([Q[state][a] for a in range(num_actions)])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_q = max([Q[next_state][a] for a in range(num_actions)] if not done else [0])\n",
    "            Q[state][action] += alpha * (reward + gamma * next_q - Q[state][action])\n",
    "            state = next_state\n",
    "    \n",
    "    pi = defaultdict(lambda: 0)\n",
    "    for state in Q:\n",
    "        pi[state] = np.argmax([Q[state][a] for a in range(num_actions)])\n",
    "    return pi, Q\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = GridWorld()\n",
    "    pi, Q = q_learning(env)\n",
    "    print(\"Learned Policy:\", {s: pi[s] for s in sorted(pi.keys())})\n",
    "    # Example episode\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = pi[state]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        print(f\"Action: {action}, Reward: {reward}\")\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a046ee-4b5e-4737-905f-a237dc12d16a",
   "metadata": {},
   "source": [
    "# chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04aaf1e1-5e3a-430c-b3ad-f156f4f0636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy,env,alpha=0.1,gamma=0.9,num_episodes=1000):\n",
    "    V=np.zeros(env.observation_space.n)\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset()\n",
    "        done =False\n",
    "        while not done:\n",
    "            action=policy[state]\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            V[state]+= alpha*(reward+gamma*V[next_state]-V[state])\n",
    "            state=next_state\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2b4b9bf-149a-45bd-9b50-6702104e530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = [\n",
    "    [('A', 0), ('B', 0)],\n",
    "    [('B', 1)],\n",
    "    [('B', 1)],\n",
    "    [('B', 1)],\n",
    "    [('B', 1)],\n",
    "    [('B', 1)],\n",
    "    [('B', 1)],\n",
    "    [('B', 0)],\n",
    "]\n",
    "\n",
    "states = ['A', 'B']\n",
    "\n",
    "def monte_carlo_value_estimates(episodes):\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, reward = episode[t]\n",
    "            G = reward + G\n",
    "            if state not in visited:\n",
    "                returns[state].append(G)\n",
    "                visited.add(state)\n",
    "\n",
    "    V = {s: sum(returns[s]) / len(returns[s]) if returns[s] else 0 for s in states}\n",
    "    return V\n",
    "\n",
    "def td0_batch_update(episodes,max_iterations=1000,theta=1e-4):\n",
    "    V={s:0 for s in states}\n",
    "    for iteration in range(max_iterations):\n",
    "        total_change=0\n",
    "        updates=defaultdict(float)\n",
    "        counts=defaultdict(int)\n",
    "\n",
    "        for episode in episodes:\n",
    "            for t in range(len(episodes)-1):\n",
    "                s,r=episode[t]\n",
    "                s_next,_=episode[t+1]\n",
    "                td_target=r+V[s_next]\n",
    "                td_error=td_target-V[s]\n",
    "                updates[s]+=td_error\n",
    "                counts[s]+=1\n",
    "            for s in updates:\n",
    "                 mean_update = updates[s] / counts[s]\n",
    "                 V[s] += mean_update\n",
    "                 total_change = max(total_change, abs(mean_update))\n",
    "           \n",
    "\n",
    "        if total_change < theta:\n",
    "            break\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca2f053-360d-4934-9bf2-4f5bd1ef259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q,state,env,epsilon):\n",
    "    if np.random.rand()<epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "def sarse(env,alpha=0.1,gamma=0.9,epsilon=0.1,num_episodes=1000):\n",
    "    Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    for episode in range(num_episdoes):\n",
    "        state=env.reset()\n",
    "        action=epsilon_greedy_policy(Q,state,env,epsilon)\n",
    "        done=False\n",
    "        while not done:\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            next_action=epsilon_greedy_policy(Q,next_state,env,epsilon)\n",
    "            Q[state,action]+=aplha*(reward+gamma*Q[next_state,next_action]-Q[state,action])\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    \n",
    "    # Derive greedy policy from Q\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63a099fb-85db-47e9-9c23-38e3f4900607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env,alpha=0.1,epsilon=0.1,gamma=0.9,num_episodes=1000):\n",
    "    Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset\n",
    "        \n",
    "        done=False\n",
    "        while not done:\n",
    "            action=epsilon_greedy_policy(Q<state,env,epsilon)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            Q[state,action]+=alpha*(reward+gamma*np.max(Q[next_state])-Q[state,action])\n",
    "            state=next_state\n",
    "\n",
    "    policy=np.argmax(Q,axis=1)\n",
    "    return Q,policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46353d1-4b69-47d0-8b6c-908fde71d174",
   "metadata": {},
   "source": [
    "# expected sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79e50091-1318-4491-9983-2ebbd6d73b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_prob(Q,state,env,epsilon):\n",
    "    num_actions=env.action_space.n\n",
    "    probs=np.ones(num_actions)*epsilon/num_actions\n",
    "    best_action=np.argmax(Q[state])\n",
    "    probs[best_action]+=(1-epsilon)\n",
    "    return probs\n",
    "\n",
    "def expected_sarsa(env,alpha=0.1,gamma=0.9,epsilon=0.1,num_episodes=1000):\n",
    "    Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset()\n",
    "       \n",
    "        done=False\n",
    "        while not done:\n",
    "            action=epsilon_greedy_policy(Q,state,env,epsilon)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            next_action=epsilon_greedy_policy(Q,next_state,env,epsilon)\n",
    "            action_prob=get_action_prob(Q,next_state,env,epsilon)\n",
    "            action_sum=np.sum(action_prob*Q[next_state])\n",
    "            Q[state,action]+=alpha*(reward+action_sum - Q[state,action])\n",
    "            state = next_state\n",
    "    \n",
    "    # Derive greedy policy from Q\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8dc4cce-6169-4ea8-aaef-c0230462b942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Environment settings\n",
    "HEIGHT = 4\n",
    "WIDTH = 12\n",
    "START = (3, 0)\n",
    "GOAL = (3, 11)\n",
    "CLIFF = [(3, i) for i in range(1, 11)]\n",
    "ACTIONS = ['U', 'D', 'L', 'R']\n",
    "ACTION_MAP = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}\n",
    "\n",
    "# Parameters\n",
    "EPISODES = 5000\n",
    "EPSILON = 0.01\n",
    "ALPHA = 0.1\n",
    "GAMMA = 1.0\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Take an action in the environment and return new state, reward, and terminal status.\"\"\"\n",
    "    i, j = state\n",
    "    di, dj = ACTION_MAP[action]\n",
    "    ni, nj = max(0, min(HEIGHT - 1, i + di)), max(0, min(WIDTH - 1, j + dj))\n",
    "    next_state = (ni, nj)\n",
    "    reward = -1\n",
    "    done = False\n",
    "    if state == GOAL:\n",
    "        return state, 0, True\n",
    "    if next_state in CLIFF:\n",
    "        return START, -100, False\n",
    "    if next_state == GOAL:\n",
    "        return next_state, 0, True\n",
    "    return next_state, reward, False\n",
    "\n",
    "def choose_action(state, Q, epsilon):\n",
    "    \"\"\"ε-greedy action selection.\"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(ACTIONS)\n",
    "    q_vals = np.array([Q[state][a] for a in ACTIONS])\n",
    "    return ACTIONS[np.argmax(q_vals)]\n",
    "\n",
    "def sarsa():\n",
    "    \"\"\"SARSA agent.\"\"\"\n",
    "    Q = defaultdict(lambda: {a: 0.0 for a in ACTIONS})\n",
    "    rewards = []\n",
    "    for ep in range(EPISODES):\n",
    "        state = START\n",
    "        action = choose_action(state, Q, EPSILON)\n",
    "        total_reward = 0\n",
    "        while state != GOAL:\n",
    "            next_state, reward, done = step(state, action)\n",
    "            next_action = choose_action(next_state, Q, EPSILON)\n",
    "            Q[state][action] += ALPHA * (reward + GAMMA * Q[next_state][next_action] - Q[state][action])\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "def q_learning():\n",
    "    \"\"\"Q-learning agent.\"\"\"\n",
    "    Q = defaultdict(lambda: {a: 0.0 for a in ACTIONS})\n",
    "    rewards = []\n",
    "    for ep in range(EPISODES):\n",
    "        state = START\n",
    "        total_reward = 0\n",
    "        while state != GOAL:\n",
    "            action = choose_action(state, Q, EPSILON)\n",
    "            next_state, reward, done = step(state, action)\n",
    "            best_next = max(Q[next_state].values())\n",
    "            Q[state][action] += ALPHA * (reward + GAMMA * best_next - Q[state][action])\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward)\n",
    "    return rewards\n",
    "\n",
    "# Run both agents\n",
    "sarsa_rewards = sarsa()\n",
    "q_learning_rewards = q_learning()\n",
    "\n",
    "# Smooth rewards\n",
    "def moving_average(data, window=10):\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot results\n",
    "plt.plot(moving_average(sarsa_rewards), label=\"Sarsa\", color='blue')\n",
    "plt.plot(moving_average(q_learning_rewards), label=\"Q-learning\", color='red')\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of rewards during episode\")\n",
    "plt.title(\"Cliff Walking: Sarsa vs Q-learning\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "badb92b3-0415-49ab-a2cb-239751511b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q1,Q2,state,env,epsilon):\n",
    "    if np.random.rand()<epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        Q_combined=Q1[state]+Q2[state]\n",
    "        return np.argmax(Q_combined)\n",
    "\n",
    "def double_q_learning(env,alpha=0.1,gamma=0.9,epsilon=0.1,num_episodes=1000):\n",
    "    Q1 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q2 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize state\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Loop until episode terminates\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Choose action using epsilon-greedy policy based on Q1 + Q2\n",
    "            action = epsilon_greedy_policy(Q1, Q2, state, env, epsilon)\n",
    "            \n",
    "            # Take action, observe reward and next state\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Update Q1 or Q2 with 0.5 probability\n",
    "            if np.random.rand() < 0.5:\n",
    "                # Update Q1: Q1(S, A) ← Q1(S, A) + α[R + γ Q2(S', argmax_a Q1(S', a)) - Q1(S, A)]\n",
    "                best_next_action = np.argmax(Q1[next_state])\n",
    "                Q1[state, action] += alpha * (reward + gamma * Q2[next_state, best_next_action] - Q1[state, action])\n",
    "            else:\n",
    "                # Update Q2: Q2(S, A) ← Q2(S, A) + α[R + γ Q1(S', argmax_a Q2(S', a)) - Q2(S, A)]\n",
    "                best_next_action = np.argmax(Q2[next_state])\n",
    "                Q2[state, action] += alpha * (reward + gamma * Q1[next_state, best_next_action] - Q2[state, action])\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "    \n",
    "    # Derive greedy policy from Q1 + Q2\n",
    "    Q_combined = Q1 + Q2\n",
    "    policy = np.argmax(Q_combined, axis=1)\n",
    "    \n",
    "    return Q1, Q2, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c2504e-123d-4897-9a31-5febf2048318",
   "metadata": {},
   "source": [
    "# Chapter 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9847cba5-6948-4aef-8992-7bf25c46e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_td(env,policy,alpha=0.1,gamma=0.9,n=5,num_episodes=10000):\n",
    "    V=np.zeros(env.observation_space.n)\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset()\n",
    "        states=[state]\n",
    "        rewards=[0]\n",
    "        T=float('inf')\n",
    "        t=0\n",
    "        while True:\n",
    "            if t<T:\n",
    "                action=policy[state]\n",
    "                next_state,reward,done,_=env.step(action)\n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    T=t+1\n",
    "            tau=t-n-1\n",
    "            if tau>=0:\n",
    "                G=0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                     G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                if tau+n<T:\n",
    "                    G += (gamma ** n) * V[states[tau + n]]\n",
    "                V[states[tau]]+=alpha*(G-V[states[tau]])\n",
    "            t += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # Break when tau reaches T-1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaafb2c5-39b3-481c-8502-4a230908488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q,state,env,epsilon):\n",
    "    if np.random.rand()<epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "\n",
    "def n_step_sarsa(env,alpha=0.1,gamma=0.9,epsilon=0.1,n=5,num_episodes=1000):\n",
    "    Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset()\n",
    "        action=epsilon_greedy_policy(Q,state,env,epsilon)\n",
    "        states=[state]\n",
    "        actions=[action]\n",
    "        rewards=[0]\n",
    "        T=float('inf')\n",
    "        t=0\n",
    "        while True:\n",
    "            if t<T:\n",
    "                next_state,reward,done,_=env.step(action)\n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    T=t+1\n",
    "                else:\n",
    "                    next_action=epsilon_greedy_policy(Q,next_state,env,epsilon)\n",
    "                    actions.append(next_action)\n",
    "            tau=t-n+1\n",
    "            if tau>=0:\n",
    "                G=0\n",
    "                for i in range(tau+1,min(tau+n,T)+1):\n",
    "                    G+=(gamma**(i-tau-1))*rewards[i]\n",
    "                if tau+n<T:\n",
    "                    G+=(gamma**n)*Q[states[tau+n],actions[tau+n]]\n",
    "                Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
    "                \n",
    "                # Ensure π(·|S_tau) is epsilon-greedy wrt Q (implicitly handled by policy function)\n",
    "            \n",
    "            # Move to next time step\n",
    "            t += 1\n",
    "            \n",
    "            # Break when tau reaches T-1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    # Derive final epsilon-greedy policy\n",
    "    policy = np.argmax(Q, axis=1)  # Greedy policy for evaluation\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5450267c-0a50-4e8a-a7ae-f5d5e7eeee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_policy(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Returns an action using epsilon-greedy policy based on Q-values.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability (0 <= epsilon <= 1)\n",
    "    \n",
    "    Returns:\n",
    "        action: Selected action\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Greedy action\n",
    "\n",
    "def get_action_probabilities(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Returns action probabilities under epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability (0 <= epsilon <= 1)\n",
    "    \n",
    "    Returns:\n",
    "        probs: Array of shape [n_actions] with action probabilities\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    probs = np.ones(n_actions) * epsilon / n_actions  # Uniform probability for exploration\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += (1.0 - epsilon)  # Add (1-ε) probability for greedy action\n",
    "    return probs\n",
    "\n",
    "def n_step_expected_sarsa(env, alpha=0.1, gamma=0.9, epsilon=0.1, n=3, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    N-step Expected SARSA algorithm for estimating Q-function.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment (Frozen Lake)\n",
    "        alpha: Step size (0 < alpha <= 1)\n",
    "        gamma: Discount factor (0 <= gamma <= 1)\n",
    "        epsilon: Exploration probability for epsilon-greedy policy\n",
    "        n: Number of steps for n-step Expected SARSA\n",
    "        num_episodes: Number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        policy: Greedy policy derived from Q\n",
    "    \"\"\"\n",
    "    # Initialize Q arbitrarily (except Q(terminal, ·) = 0)\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize and store S0 (non-terminal)\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Select and store A0 ~ π(·|S0)\n",
    "        action = epsilon_greedy_policy(Q, state, env, epsilon)\n",
    "        \n",
    "        # Storage for states, actions, and rewards\n",
    "        states = [state]  # S_0, S_1, ..., S_t\n",
    "        actions = [action]  # A_0, A_1, ..., A_t\n",
    "        rewards = [0]  # R_1, R_2, ..., R_t+1 (R_0 is dummy)\n",
    "        \n",
    "        T = float('inf')  # Time at which episode terminates\n",
    "        t = 0  # Current time step\n",
    "        \n",
    "        while True:\n",
    "            # If t < T, take action and observe\n",
    "            if t < T:\n",
    "                # Take action A_t\n",
    "                next_state, reward, done, _ = env.step(actions[t])\n",
    "                \n",
    "                # Store next reward and state\n",
    "                rewards.append(reward)\n",
    "                states.append(next_state)\n",
    "                \n",
    "                if done:\n",
    "                    # If S_{t+1} is terminal, set T\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Select and store A_{t+1} ~ π(·|S_{t+1})\n",
    "                    next_action = epsilon_greedy_policy(Q, next_state, env, epsilon)\n",
    "                    actions.append(next_action)\n",
    "            \n",
    "            # Time whose estimate is being updated\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            # If tau >= 0, update Q(S_tau, A_tau)\n",
    "            if tau >= 0:\n",
    "                # Compute n-step return G\n",
    "                G = 0\n",
    "                # Sum rewards from tau+1 to min(tau+n, T)\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                \n",
    "                # If tau + n < T, add expected value: γ^n * Σ_a π(a|S_{tau+n}) Q(S_{tau+n}, a)\n",
    "                if tau + n < T:\n",
    "                    action_probs = get_action_probabilities(Q, states[tau + n], env, epsilon)\n",
    "                    expected_value = np.sum(action_probs * Q[states[tau + n]])\n",
    "                    G += (gamma ** n) * expected_value\n",
    "                \n",
    "                # Update Q(S_tau, A_tau)\n",
    "                Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
    "                \n",
    "                # Ensure π(·|S_tau) is epsilon-greedy wrt Q (implicitly handled by policy function)\n",
    "            \n",
    "            # Move to next time step\n",
    "            t += 1\n",
    "            \n",
    "            # Break when tau reaches T-1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    # Derive final greedy policy for evaluation\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ee2ff7d-ebd1-49ed-95ed-b31d83ad5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_policy(env):\n",
    "    return env.action_space.sample()\n",
    "def get_behavior_policy_prob(env):\n",
    "    return 1/env.action_space.n\n",
    "def target_policy(Q,state):\n",
    "    return np.argmax(Q[state])\n",
    "def get_target_policy_prob(Q,state,action):\n",
    "    greedy_action=np.argmax(Q[state])\n",
    "    return 1.0 if action==greedy_action else 0.0\n",
    "def off_policy_n_step_sarsa(env,alpha=0.1,gamma=0.9,n=5,num_episodes=1000):\n",
    "    Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    for episode in range(num_episodes):\n",
    "        states=env.reset()\n",
    "        action=behavior_policy(env)\n",
    "        states=[state]\n",
    "        actions=[action]\n",
    "        rewards=[0]\n",
    "        T=float('inf')\n",
    "        t=0\n",
    "        while True:\n",
    "           if t<T:\n",
    "                next_state,reward,done,_=env.step(action)\n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    T=t+1\n",
    "                else:\n",
    "                    next_action=behavior_policy(env)\n",
    "                    actions.append(next_action)\n",
    "        tau=t-n+1\n",
    "        if tau >= 0:\n",
    "                # Compute importance sampling ratio ρ\n",
    "                rho = 1.0\n",
    "                # Product from i = tau+1 to min(tau+n-1, T-1)\n",
    "                for i in range(tau + 1, min(tau + n - 1, T - 1) + 1):\n",
    "                    pi_prob = get_target_policy_prob(Q, states[i], actions[i])\n",
    "                    b_prob = get_behavior_policy_prob(env)\n",
    "                    rho *= pi_prob / b_prob\n",
    "                \n",
    "                # Compute n-step return G\n",
    "                G = 0\n",
    "                # Sum rewards from tau+1 to min(tau+n, T)\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                \n",
    "                # If tau + n < T, add discounted Q(S_{tau+n}, A_{tau+n})\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * Q[states[tau + n], actions[tau + n]]\n",
    "                \n",
    "                # Update Q(S_tau, A_tau) with importance sampling\n",
    "                Q[states[tau], actions[tau]] += alpha * rho * (G - Q[states[tau], actions[tau]])\n",
    "                \n",
    "                # Ensure π(·|S_tau) is greedy wrt Q (handled by target_policy)\n",
    "            \n",
    "            # Move to next time step\n",
    "        t += 1\n",
    "            \n",
    "            # Break when tau reaches T-1\n",
    "        if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    # Derive final greedy policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20975a55-e19c-4d3b-98e8-72643107365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def behavior_policy(env):\n",
    "    \"\"\"\n",
    "    Returns an action using a uniform random policy (behavior policy b).\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "    \n",
    "    Returns:\n",
    "        action: Random action\n",
    "    \"\"\"\n",
    "    return env.action_space.sample()\n",
    "\n",
    "def get_behavior_policy_prob(env):\n",
    "    \"\"\"\n",
    "    Returns the probability of an action under the uniform random behavior policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "    \n",
    "    Returns:\n",
    "        prob: Probability of selecting any action\n",
    "    \"\"\"\n",
    "    return 1.0 / env.action_space.n  # Uniform over all actions\n",
    "\n",
    "def target_policy(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Returns an action using an epsilon-greedy policy (target policy π) based on Q-values.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability (0 <= epsilon <= 1)\n",
    "    \n",
    "    Returns:\n",
    "        action: Selected action\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def get_target_policy_probs(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Returns action probabilities under the epsilon-greedy target policy π.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability (0 <= epsilon <= 1)\n",
    "    \n",
    "    Returns:\n",
    "        probs: Array of shape [n_actions] with action probabilities\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    probs = np.ones(n_actions) * epsilon / n_actions\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += (1.0 - epsilon)\n",
    "    return probs\n",
    "\n",
    "def compute_v_bar(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Computes V_bar(s) = Σ_a π(a|s) Q(s, a) under the target policy π.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability for target policy\n",
    "    \n",
    "    Returns:\n",
    "        v_bar: Expected Q-value under π\n",
    "    \"\"\"\n",
    "    action_probs = get_target_policy_probs(Q, state, env, epsilon)\n",
    "    return np.sum(action_probs * Q[state])\n",
    "\n",
    "def compute_per_decision_return(t, h, T, states, actions, rewards, Q, gamma, env, epsilon):\n",
    "    \"\"\"\n",
    "    Recursively computes the per-decision n-step return G_{t:h} with control variates.\n",
    "    \n",
    "    Args:\n",
    "        t: Start time of the return\n",
    "        h: Horizon time (t < h <= T)\n",
    "        T: Terminal time\n",
    "        states: List of states S_0, S_1, ...\n",
    "        actions: List of actions A_0, A_1, ...\n",
    "        rewards: List of rewards R_1, R_2, ...\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        gamma: Discount factor\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability for target policy\n",
    "    \n",
    "    Returns:\n",
    "        G: N-step return G_{t:h}\n",
    "    \"\"\"\n",
    "    # Base cases\n",
    "    if t == h:\n",
    "        return Q[states[h], actions[h]]  # G_{h:h} = Q_{h-1}(S_h, A_h)\n",
    "    if t == T - 1 and h >= T:\n",
    "        return rewards[T]  # G_{T-1:h} = R_T\n",
    "    \n",
    "    # Compute importance sampling ratio ρ_{t+1}\n",
    "    pi_prob = get_target_policy_probs(Q, states[t + 1], env, epsilon)[actions[t + 1]]\n",
    "    b_prob = get_behavior_policy_prob(env)\n",
    "    rho = pi_prob / b_prob if b_prob > 0 else 0.0\n",
    "    \n",
    "    # Compute V_bar(S_{t+1})\n",
    "    v_bar = compute_v_bar(Q, states[t + 1], env, epsilon)\n",
    "    \n",
    "    # Recursively compute G_{t+1:h}\n",
    "    G_next = compute_per_decision_return(t + 1, h, T, states, actions, rewards, Q, gamma, env, epsilon)\n",
    "    \n",
    "    # G_{t:h} = R_{t+1} + γ ρ_{t+1} (G_{t+1:h} - Q(S_{t+1}, A_{t+1})) + γ V_bar(S_{t+1})\n",
    "    G = rewards[t + 1] + gamma * rho * (G_next - Q[states[t + 1], actions[t + 1]]) + gamma * v_bar\n",
    "    return G\n",
    "\n",
    "def per_decision_n_step_sarsa(env, alpha=0.1, gamma=0.9, epsilon=0.1, n=3, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Off-policy n-step SARSA with per-decision importance sampling and control variates.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment (Frozen Lake)\n",
    "        alpha: Step size (0 < alpha <= 1)\n",
    "        gamma: Discount factor (0 <= gamma <= 1)\n",
    "        epsilon: Exploration probability for target policy\n",
    "        n: Number of steps\n",
    "        num_episodes: Number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        policy: Greedy policy derived from Q\n",
    "    \"\"\"\n",
    "    # Initialize Q arbitrarily (except Q(terminal, ·) = 0)\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize and store S0 (non-terminal)\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Select and store A0 ~ b(·|S0)\n",
    "        action = behavior_policy(env)\n",
    "        \n",
    "        # Storage for states, actions, and rewards\n",
    "        states = [state]  # S_0, S_1, ..., S_t\n",
    "        actions = [action]  # A_0, A_1, ..., A_t\n",
    "        rewards = [0]  # R_1, R_2, ..., R_t+1 (R_0 is dummy)\n",
    "        \n",
    "        T = float('inf')  # Time at which episode terminates\n",
    "        t = 0  # Current time step\n",
    "        \n",
    "        while True:\n",
    "            # If t < T, take action and observe\n",
    "            if t < T:\n",
    "                # Take action A_t\n",
    "                next_state, reward, done, _ = env.step(actions[t])\n",
    "                \n",
    "                # Store next reward and state\n",
    "                rewards.append(reward)\n",
    "                states.append(next_state)\n",
    "                \n",
    "                if done:\n",
    "                    # If S_{t+1} is terminal, set T\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Select and store A_{t+1} ~ b(·|S_{t+1})\n",
    "                    next_action = behavior_policy(env)\n",
    "                    actions.append(next_action)\n",
    "            \n",
    "            # Time whose estimate is being updated\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            # If tau >= 0, update Q(S_tau, A_tau)\n",
    "            if tau >= 0:\n",
    "                # Compute horizon h = tau + n\n",
    "                h = tau + n\n",
    "                \n",
    "                # Compute per-decision n-step return G_{tau:h}\n",
    "                G = compute_per_decision_return(tau, h, T, states, actions, rewards, Q, gamma, env, epsilon)\n",
    "                \n",
    "                # Update Q(S_tau, A_tau)\n",
    "                Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
    "            \n",
    "            # Move to next time step\n",
    "            t += 1\n",
    "            \n",
    "            # Break when tau reaches T-1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    # Derive final greedy policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b26816ca-ed15-4217-9dd4-843f3eeaf48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def behavior_policy(env):\n",
    "    \"\"\"\n",
    "    Returns an action using a uniform random policy (behavior policy).\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "    \n",
    "    Returns:\n",
    "        action: Random action\n",
    "    \"\"\"\n",
    "    return env.action_space.sample()\n",
    "\n",
    "def target_policy(Q, state):\n",
    "    \"\"\"\n",
    "    Returns an action using a greedy policy (target policy π) based on Q-values.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "    \n",
    "    Returns:\n",
    "        action: Greedy action\n",
    "    \"\"\"\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "def get_target_policy_probs(Q, state, env):\n",
    "    \"\"\"\n",
    "    Returns action probabilities under the greedy target policy π.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "    \n",
    "    Returns:\n",
    "        probs: Array of shape [n_actions] with action probabilities\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    probs = np.zeros(n_actions)\n",
    "    greedy_action = np.argmax(Q[state])\n",
    "    probs[greedy_action] = 1.0  # Greedy policy: 1 for best action, 0 otherwise\n",
    "    return probs\n",
    "\n",
    "def n_step_tree_backup(env, alpha=0.1, gamma=0.9, n=3, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    N-step Tree Backup algorithm for estimating Q-function.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment (Frozen Lake)\n",
    "        alpha: Step size (0 < alpha <= 1)\n",
    "        gamma: Discount factor (0 <= gamma <= 1)\n",
    "        n: Number of steps for n-step Tree Backup\n",
    "        num_episodes: Number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        policy: Greedy policy derived from Q\n",
    "    \"\"\"\n",
    "    # Initialize Q arbitrarily (except Q(terminal, ·) = 0)\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize and store S0 (non-terminal)\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Choose and store A0 arbitrarily (using behavior policy)\n",
    "        action = behavior_policy(env)\n",
    "        \n",
    "        # Storage for states, actions, and rewards\n",
    "        states = [state]  # S_0, S_1, ..., S_t\n",
    "        actions = [action]  # A_0, A_1, ..., A_t\n",
    "        rewards = [0]  # R_1, R_2, ..., R_t+1 (R_0 is dummy)\n",
    "        \n",
    "        T = float('inf')  # Time at which episode terminates\n",
    "        t = 0  # Current time step\n",
    "        \n",
    "        while True:\n",
    "            # If t < T, take action and observe\n",
    "            if t < T:\n",
    "                # Take action A_t\n",
    "                next_state, reward, done, _ = env.step(actions[t])\n",
    "                \n",
    "                # Store next reward and state\n",
    "                rewards.append(reward)\n",
    "                states.append(next_state)\n",
    "                \n",
    "                if done:\n",
    "                    # If S_{t+1} is terminal, set T\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Choose and store A_{t+1} arbitrarily (using behavior policy)\n",
    "                    next_action = behavior_policy(env)\n",
    "                    actions.append(next_action)\n",
    "            \n",
    "            # Time whose estimate is being updated (corrected: τ = t - n + 1)\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            # If tau >= 0, update Q(S_tau, A_tau)\n",
    "            if tau >= 0:\n",
    "                # Compute the return G\n",
    "                if t + 1 >= T:\n",
    "                    # If t + 1 >= T, G = R_T\n",
    "                    G = rewards[T]\n",
    "                else:\n",
    "                    # Otherwise, G = R_{t+1} + γ Σ_a π(a|S_{t+1}) Q(S_{t+1}, a)\n",
    "                    action_probs = get_target_policy_probs(Q, states[t + 1], env)\n",
    "                    G = rewards[t + 1] + gamma * np.sum(action_probs * Q[states[t + 1]])\n",
    "                \n",
    "                # Loop for k from min(t, T-1) down to tau + 1\n",
    "                for k in range(min(t, T - 1), tau, -1):\n",
    "                    # G = R_k + γ [ Σ_{a ≠ A_k} π(a|S_k) Q(S_k, a) + π(A_k|S_k) G ]\n",
    "                    action_probs = get_target_policy_probs(Q, states[k], env)\n",
    "                    ak = actions[k]\n",
    "                    sum_other_actions = 0.0\n",
    "                    for a in range(env.action_space.n):\n",
    "                        if a != ak:\n",
    "                            sum_other_actions += action_probs[a] * Q[states[k], a]\n",
    "                    G = rewards[k] + gamma * (sum_other_actions + action_probs[ak] * G)\n",
    "                \n",
    "                # Update Q(S_tau, A_tau)\n",
    "                Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
    "                \n",
    "                # Ensure π(·|S_tau) is greedy wrt Q (handled by target_policy)\n",
    "            \n",
    "            # Move to next time step\n",
    "            t += 1\n",
    "            \n",
    "            # Break when tau reaches T-1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    # Derive final greedy policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dec8d82-756d-4db8-9abc-0e5cecd12124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def behavior_policy(env):\n",
    "    \"\"\"\n",
    "    Returns an action using a uniform random policy (behavior policy b).\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "    \n",
    "    Returns:\n",
    "        action: Random action\n",
    "    \"\"\"\n",
    "    return env.action_space.sample()\n",
    "\n",
    "def get_behavior_policy_prob(env):\n",
    "    \"\"\"\n",
    "    Returns the probability of an action under the uniform random behavior policy.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment\n",
    "    \n",
    "    Returns:\n",
    "        prob: Probability of selecting any action\n",
    "    \"\"\"\n",
    "    return 1.0 / env.action_space.n  # Uniform over all actions\n",
    "\n",
    "def target_policy(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Returns an action using an epsilon-greedy policy (target policy π) based on Q-values.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability (0 <= epsilon <= 1)\n",
    "    \n",
    "    Returns:\n",
    "        action: Selected action\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def get_target_policy_probs(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Returns action probabilities under the epsilon-greedy target policy π.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability (0 <= epsilon <= 1)\n",
    "    \n",
    "    Returns:\n",
    "        probs: Array of shape [n_actions] with action probabilities\n",
    "    \"\"\"\n",
    "    n_actions = env.action_space.n\n",
    "    probs = np.ones(n_actions) * epsilon / n_actions\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += (1.0 - epsilon)\n",
    "    return probs\n",
    "\n",
    "def compute_v_bar(Q, state, env, epsilon):\n",
    "    \"\"\"\n",
    "    Computes V_bar(s) = Σ_a π(a|s) Q(s, a) under the target policy π.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        state: Current state\n",
    "        env: Gym environment\n",
    "        epsilon: Exploration probability for target policy\n",
    "    \n",
    "    Returns:\n",
    "        v_bar: Expected Q-value under π\n",
    "    \"\"\"\n",
    "    action_probs = get_target_policy_probs(Q, state, env, epsilon)\n",
    "    return np.sum(action_probs * Q[state])\n",
    "\n",
    "def off_policy_n_step_q_sigma(env, alpha=0.1, gamma=0.9, epsilon=0.1, n=3, sigma=0.5, num_episodes=1000):\n",
    "    \"\"\"\n",
    "    Off-policy n-step Q(σ) algorithm for estimating Q-function.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment (Frozen Lake)\n",
    "        alpha: Step size (0 < alpha <= 1)\n",
    "        gamma: Discount factor (0 <= gamma <= 1)\n",
    "        epsilon: Exploration probability for target policy\n",
    "        n: Number of steps\n",
    "        sigma: Interpolation parameter (0 <= sigma <= 1)\n",
    "        num_episodes: Number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        policy: Greedy policy derived from Q\n",
    "    \"\"\"\n",
    "    # Initialize Q arbitrarily (except Q(terminal, ·) = 0)\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize and store S0 (non-terminal)\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Choose and store A0 ~ b(·|S0)\n",
    "        action = behavior_policy(env)\n",
    "        \n",
    "        # Storage for states, actions, rewards, and importance sampling ratios\n",
    "        states = [state]  # S_0, S_1, ..., S_t\n",
    "        actions = [action]  # A_0, A_1, ..., A_t\n",
    "        rewards = [0]  # R_1, R_2, ..., R_t+1 (R_0 is dummy)\n",
    "        rhos = [1.0]  # ρ_1, ρ_2, ..., ρ_t+1 (ρ_0 is dummy)\n",
    "        \n",
    "        T = float('inf')  # Time at which episode terminates\n",
    "        t = 0  # Current time step\n",
    "        \n",
    "        while True:\n",
    "            # If t < T, take action and observe\n",
    "            if t < T:\n",
    "                # Take action A_t\n",
    "                next_state, reward, done, _ = env.step(actions[t])\n",
    "                \n",
    "                # Store next reward and state\n",
    "                rewards.append(reward)\n",
    "                states.append(next_state)\n",
    "                \n",
    "                if done:\n",
    "                    # If S_{t+1} is terminal, set T\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Choose and store A_{t+1} ~ b(·|S_{t+1})\n",
    "                    next_action = behavior_policy(env)\n",
    "                    actions.append(next_action)\n",
    "                    \n",
    "                    # Compute and store ρ_{t+1} = π(A_{t+1} | S_{t+1}) / b(A_{t+1} | S_{t+1})\n",
    "                    pi_prob = get_target_policy_probs(Q, next_state, env, epsilon)[next_action]\n",
    "                    b_prob = get_behavior_policy_prob(env)\n",
    "                    rho = pi_prob / b_prob if b_prob > 0 else 0.0\n",
    "                    rhos.append(rho)\n",
    "            \n",
    "            # Time whose estimate is being updated\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            # If tau >= 0, update Q(S_tau, A_tau)\n",
    "            if tau >= 0:\n",
    "                # Compute the return G\n",
    "                G = 0.0\n",
    "                # Loop for k from min(t+1, T) down to tau + 1\n",
    "                for k in range(min(t + 1, T), tau, -1):\n",
    "                    if k == T:\n",
    "                        # If k = T, G = R_T\n",
    "                        G = rewards[T]\n",
    "                    else:\n",
    "                        # Compute V_bar(S_k) = Σ_a π(a|S_k) Q(S_k, a)\n",
    "                        v_bar = compute_v_bar(Q, states[k], env, epsilon)\n",
    "                        \n",
    "                        # G = R_k + σ_k ρ_k (G - Q(S_k, A_k)) + (1 - σ_k) π(A_k|S_k) (G - Q(S_k, A_k)) + V_bar\n",
    "                        pi_prob_k = get_target_policy_probs(Q, states[k], env, epsilon)[actions[k]]\n",
    "                        delta = G - Q[states[k], actions[k]]\n",
    "                        G = rewards[k] + gamma * (sigma * rhos[k] * delta + (1 - sigma) * pi_prob_k * delta + v_bar)\n",
    "                \n",
    "                # Update Q(S_tau, A_tau)\n",
    "                Q[states[tau], actions[tau]] += alpha * (G - Q[states[tau], actions[tau]])\n",
    "                \n",
    "                # Ensure π(·|S_tau) is epsilon-greedy wrt Q (handled implicitly by target_policy)\n",
    "            \n",
    "            # Move to next time step\n",
    "            t += 1\n",
    "            \n",
    "            # Break when tau reaches T-1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    # Derive final greedy policy for evaluation\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f3dd0a-36a6-4db3-8676-801c286308fb",
   "metadata": {},
   "source": [
    "# Chapter8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84c9fe1-8a58-4e65-bafb-0cc886fff240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[20.09580961 13.06026262 14.50858161 21.82383358]\n",
      " [15.51143535 21.74870259 14.17920486 21.26197728]\n",
      " [13.47924556 17.98409693 15.79222692 17.18103814]\n",
      " [19.5765761  14.82254312 19.76568099 20.59517695]\n",
      " [11.22541597 18.44966213 12.82159037 21.49629879]\n",
      " [16.88220311 13.15046667 18.11429019 22.612348  ]\n",
      " [17.93092291 18.04934649 21.94989778 14.97202973]\n",
      " [20.43976728 20.16053229 15.71031263 13.34952851]\n",
      " [15.18183579 19.51339044 13.56086139 16.07920011]\n",
      " [13.41941285 18.77097382 21.90433937 18.50220163]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment (for demonstration purposes)\n",
    "class SimpleEnvironment:\n",
    "    def __init__(self, num_states=10, num_actions=4):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        # Example transition probabilities (simplified)\n",
    "        self.transitions = np.random.random((num_states, num_actions, num_states))\n",
    "        self.transitions /= self.transitions.sum(axis=2, keepdims=True)  # Normalize to probabilities\n",
    "        self.rewards = np.random.random((num_states, num_actions)) * 10  # Random rewards between 0 and 10\n",
    "\n",
    "    def sample_model(self, state, action):\n",
    "        # Sample next state based on transition probabilities\n",
    "        next_state_probs = self.transitions[state, action]\n",
    "        next_state = np.random.choice(self.num_states, p=next_state_probs)\n",
    "        reward = self.rewards[state, action]\n",
    "        return reward, next_state\n",
    "\n",
    "# Random-sample one-step tabular Q-planning\n",
    "def q_planning(num_states, num_actions, alpha=0.1, gamma=0.9, iterations=1000):\n",
    "    # Initialize Q-table with zeros\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    env = SimpleEnvironment(num_states, num_actions)\n",
    "\n",
    "    # Loop forever (or for a specified number of iterations)\n",
    "    for _ in range(iterations):\n",
    "        # 1. Select a random state and action\n",
    "        state = np.random.randint(0, num_states)\n",
    "        action = np.random.randint(0, num_actions)\n",
    "        \n",
    "        # 2. Get sample next reward and next state from the sample model\n",
    "        reward, next_state = env.sample_model(state, action)\n",
    "        \n",
    "        # 3. Apply one-step Q-learning update\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    num_states = 10\n",
    "    num_actions = 4\n",
    "    Q_table = q_planning(num_states, num_actions, alpha=0.1, gamma=0.9, iterations=1000)\n",
    "    print(\"Learned Q-table:\")\n",
    "    print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12974cab-fa02-40ee-b656-1f672a5ed20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf119ec1-2290-4914-8ea4-7f02d8b4d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q,state,env,epsilon):\n",
    "    if np.random.rand()<epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def tabular_dyna_q(env,alpha=0.1,gamma=0.9,epsilon=0.1,n_plannings=5,num_episodes=1000):\n",
    "    Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    model={}\n",
    "    observed_pairs=set()\n",
    "    for episode in range(num_episodes):\n",
    "        state=env.reset()\n",
    "        while True:\n",
    "            action=epsilon_greedy_policy(Q,state,env,epsilon)\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            Q[state,action]+=alpha*(reward+gamma*np.max(Q[next_state])-Q[state,action])\n",
    "            model[(state,action)]=(reward,next_state)\n",
    "            observed_pairs.add((state,action))\n",
    "            for _ in range(n_plannings):\n",
    "                if not observed_pairs:\n",
    "                    continue\n",
    "                state_sample,action_sample=list(observed_pairs)[np.random.randit(len(observed_pairs))]\n",
    "                reward_sample,next_state_sample=model[(state_sample,action_sample)]\n",
    "                Q[state_sample,action_sample]+=alpha*(reward_sample+gamma*np.max(Q[next_state_sample])-Q[state_sample,action_sample])\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    # Derive final greedy policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bccd94c-857c-4d5a-bef4-af82469b4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predecessors(model, state, env):\n",
    "    \"\"\"\n",
    "    Returns all state-action pairs predicted to lead to the given state.\n",
    "    \n",
    "    Args:\n",
    "        model: Dictionary mapping (state, action) to (reward, next_state)\n",
    "        state: Target state to find predecessors for\n",
    "        env: Gym environment\n",
    "    \n",
    "    Returns:\n",
    "        predecessors: List of (prev_state, prev_action) pairs\n",
    "    \"\"\"\n",
    "    predecessors = []\n",
    "    for prev_state in range(env.observation_space.n):\n",
    "        for prev_action in range(env.action_space.n):\n",
    "            if (prev_state, prev_action) in model:\n",
    "                _, next_state = model[(prev_state, prev_action)]\n",
    "                if next_state == state:\n",
    "                    predecessors.append((prev_state, prev_action))\n",
    "    return predecessors\n",
    "\n",
    "\n",
    "def prioritized_sweeping(env,alpha=0.1,gamma=0.9,epsilon=0.1,theta=0.01,n_plannings=5,num_epsiodes=1000):\n",
    "    Q=np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    model={}\n",
    "    pqueue=PriorityQueue()\n",
    "    for episode in range(num_episodes):\n",
    "        # (a) S ← current (nonterminal) state\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            # (b) A ← policy(S, Q) (using ε-greedy)\n",
    "            action = epsilon_greedy_policy(Q, state, env, epsilon)\n",
    "            \n",
    "            # (c) Take action A; observe resultant reward, R, and state, S'\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # (d) Model(S, A) ← R, S'\n",
    "            model[(state, action)] = (reward, next_state)\n",
    "            \n",
    "            # (e) P ← |R + γ max_a Q(S', a) - Q(S, A)|\n",
    "            priority = abs(reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            \n",
    "            # (f) If P > θ, then insert (S, A) into PQueue with priority P\n",
    "            if priority > theta:\n",
    "                # PriorityQueue expects (priority, item), but we want higher P to be processed first,\n",
    "                # so we use negative priority (since PriorityQueue pops smallest first)\n",
    "                pqueue.put((-priority, (state, action)))\n",
    "            \n",
    "            # (g) Planning: Repeat n times, while PQueue is not empty\n",
    "            for _ in range(n_planning):\n",
    "                if pqueue.empty():\n",
    "                    break\n",
    "                \n",
    "                # S, A ← first(PQueue)\n",
    "                _, (state_plan, action_plan) = pqueue.get()\n",
    "                \n",
    "                # R, S' ← Model(S, A)\n",
    "                reward_plan, next_state_plan = model[(state_plan, action_plan)]\n",
    "                \n",
    "                # Q(S, A) ← Q(S, A) + α [R + γ max_a Q(S', a) - Q(S, A)]\n",
    "                Q[state_plan, action_plan] += alpha * (\n",
    "                    reward_plan + gamma * np.max(Q[next_state_plan]) - Q[state_plan, action_plan]\n",
    "                )\n",
    "                \n",
    "                # Loop for all (S̄, Ā) predicted to lead to S\n",
    "                predecessors = get_predecessors(model, state_plan, env)\n",
    "                for prev_state, prev_action in predecessors:\n",
    "                    # R̄ ← predicted reward for (S̄, Ā, S)\n",
    "                    prev_reward, _ = model[(prev_state, prev_action)]\n",
    "                    \n",
    "                    # P ← |R̄ + γ max_a Q(S, a) - Q(S̄, Ā)|\n",
    "                    priority = abs(prev_reward + gamma * np.max(Q[state_plan]) - Q[prev_state, prev_action])\n",
    "                    \n",
    "                    # If P > θ, then insert (S̄, Ā) into PQueue with priority P\n",
    "                    if priority > theta:\n",
    "                        pqueue.put((-priority, (prev_state, prev_action)))\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    # Derive final greedy policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return Q, policy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53610bac-289d-4115-9682-97f1a4c344f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Prioritized Sweeping algorithm for a deterministic environment.\\n    \\n    Args:\\n        env: Gym environment (Frozen Lake)\\n        alpha: Step size (0 < alpha <= 1)\\n        gamma: Discount factor (0 <= gamma <= 1)\\n        epsilon: Exploration probability for epsilon-greedy policy\\n        theta: Priority threshold for PQueue\\n        n_planning: Number of planning steps per real step\\n        num_episodes: Number of episodes to run\\n    \\n    Returns:\\n        Q: Q-table of shape [n_states, n_actions]\\n        policy: Greedy policy derived from Q\\n    '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Prioritized Sweeping algorithm for a deterministic environment.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment (Frozen Lake)\n",
    "        alpha: Step size (0 < alpha <= 1)\n",
    "        gamma: Discount factor (0 <= gamma <= 1)\n",
    "        epsilon: Exploration probability for epsilon-greedy policy\n",
    "        theta: Priority threshold for PQueue\n",
    "        n_planning: Number of planning steps per real step\n",
    "        num_episodes: Number of episodes to run\n",
    "    \n",
    "    Returns:\n",
    "        Q: Q-table of shape [n_states, n_actions]\n",
    "        policy: Greedy policy derived from Q\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd2fd2b-4809-449a-9cc6-2c774df0a084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49256f12-fabb-484d-bb03-2a692288511f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233c355-6e21-480c-bf6a-112d8c776e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27791d99-9f64-462f-ad9e-bd38e1aae379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e47586-7f63-40fb-9f1d-8dfef18618ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dddca2-c49c-41f4-b84f-ed01f7518e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a529ef8-a2fa-4d9b-ac4e-7e86660d4506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cb580-e1c6-4ce0-9d02-8ed14d0b0d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d916b32-bbb9-4802-91c3-e48fa8bb167b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93f7aa-61b2-49c1-bc9c-b384556b0cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803e71c-d43d-45af-a46a-8e15d7241341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e4bee-5efe-4233-b999-8d05fa20b39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9c701-a2db-4f82-b378-3bafa40d3672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97077831-9cf6-40f9-ac25-453eedc1fca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09b417-ec3e-44ce-ba02-40c34c0c3b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4746e-8f4f-4f9d-9acb-0d103cffbdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93493f-fbcc-48f2-a599-ca57a1a18768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f6284-460a-48c6-a259-cb56e1e5bf03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da67b2-7409-45bb-83f7-3b754e57150f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf91e82-2c94-452e-9f4a-710c7da6419d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8debeb-6952-4c97-9b89-933ef6bc043e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82143bcc-1992-476e-a4c9-ebc37ebd9b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d5a91-360b-43f7-a106-122331516308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99c69e-41de-4d6f-a3d9-166cda1a5bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b10955-cb2f-4864-9777-854c4257c7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410265f5-5049-4874-852f-08e8481be76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89010c-9d3b-4a5a-b146-177ecadeecbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b1ba5-fc8d-40fc-bee0-ebceac28a524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab75f8-790a-48ec-b8ab-595388edf32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
