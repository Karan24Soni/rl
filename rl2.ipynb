{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c555c2e5-5625-4cf5-aefb-2cf5ef0a76e5",
   "metadata": {},
   "source": [
    "# chapter 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9418c9ca-6fdf-42a0-bd4f-cc3b4eb4e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gymp\n",
    "import numpy as np\n",
    "\n",
    "def gradient_monte_carlo(env, num_episodes=10000, alpha=0.01, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    \n",
    "    # Initialize weights for linear function approximator\n",
    "    w = np.zeros(num_states)  # One weight per state\n",
    "    \n",
    "    # Random policy: uniform probability over actions\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Feature vector: one-hot encoding for state s\n",
    "    def phi(state):\n",
    "        features = np.zeros(num_states)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate: v_hat(s, w) = w^T * phi(s)\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Run episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize episode\n",
    "        state, _ = env.reset()\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        \n",
    "        # Generate episode using random policy\n",
    "        while not done:\n",
    "            action_probs = random_policy()\n",
    "            action = np.random.choice(num_actions, p=action_probs)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "        \n",
    "        # Compute returns and update weights\n",
    "        G = 0\n",
    "        for t in range(len(episode_states) - 1, -1, -1):\n",
    "            state = episode_states[t]\n",
    "            reward = episode_rewards[t]\n",
    "            G = reward + gamma * G  # Compute return\n",
    "            \n",
    "            # Update weights: w = w + alpha * (G - v_hat(s, w)) * phi(s)\n",
    "            w += alpha * (G - v_hat(state, w)) * phi(state)\n",
    "    \n",
    "    return w, v_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ea1422-135d-46de-a095-2293f12aad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def semi_gradient_td0(env, num_episodes=10000, alpha=0.01, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    \n",
    "    # Initialize weights for linear function approximator\n",
    "    w = np.zeros(num_states)  # One weight per state\n",
    "    \n",
    "    # Random policy: uniform probability over actions\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Feature vector: one-hot encoding for state s\n",
    "    def phi(state):\n",
    "        features = np.zeros(num_states)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate: v_hat(s, w) = w^T * phi(s)\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize Sg\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        # Loop for each step of episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Choose A ~ π(·|S)\n",
    "            action_probs = random_policy()\n",
    "            action = np.random.choice(num_actions, p=action_probs)\n",
    "            \n",
    "            # Take action A, observe R, S'\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update weights: w ← w + α[R + γv̂(S', w) − v̂(S, w)]∇v̂(S, w)\n",
    "            target = reward + gamma * v_hat(next_state, w)\n",
    "            error = target - v_hat(state, w)\n",
    "            w += alpha * error * phi(state)\n",
    "            \n",
    "            # S ← S'\n",
    "            state = next_state\n",
    "    \n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4cda20-bc86-4932-a3c8-09136c47f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def n_step_semi_gradient_td(env, num_episodes=10000, alpha=0.01, n=3, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(num_states)\n",
    "    \n",
    "    # Random policy: uniform probability over actions\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Feature vector: one-hot encoding for state s\n",
    "    def phi(state):\n",
    "        features = np.zeros(num_states)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate: v_hat(s, w) = w^T * phi(s)\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize and store S0 != terminal\n",
    "        state, _ = env.reset()\n",
    "        states = [state]  # Store states: S0, S1, ...\n",
    "        rewards = [0]     # Store rewards: dummy R0, R1, ...\n",
    "        \n",
    "        T = float('inf')  # Time of termination\n",
    "        t = 0  # Current time step\n",
    "        \n",
    "        while True:\n",
    "            # If t < T, take action and observe next state/reward\n",
    "            if t < T:\n",
    "                action_probs = random_policy()\n",
    "                action = np.random.choice(num_actions, p=action_probs)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Store next state and reward\n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Check if terminal\n",
    "                if terminated or truncated:\n",
    "                    T = t + 1\n",
    "            \n",
    "            # Tau is the time whose estimate is being updated\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            # If tau >= 0, update weights\n",
    "            if tau >= 0:\n",
    "                # Compute G: sum of rewards from tau+1 to min(tau+n, T)\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                \n",
    "                # If tau + n < T, add gamma^n * v_hat(S_{tau+n}, w)\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * v_hat(states[tau + n], w)\n",
    "                \n",
    "                # Update weights: w ← w + α[G - v_hat(S_tau, w)]∇v_hat(S_tau, w)\n",
    "                state_tau = states[tau]\n",
    "                w += alpha * (G - v_hat(state_tau, w)) * phi(state_tau)\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "            # Until tau = T - 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44b6bdb-676a-44af-9d1b-a98cee7d4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def n_step_semi_gradient_td_fourier(env, num_episodes=10000, alpha=0.01, n=3, gamma=1.0, fourier_order=5):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    num_features = fourier_order + 1  # Order 0 to fourier_order\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(num_features)\n",
    "    \n",
    "    # Random policy: uniform probability over actions\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Fourier basis features: cos(pi * c * s_norm), s_norm in [0, 1]\n",
    "    def phi(state):\n",
    "        s_norm = state / (num_states - 1)  # Normalize state to [0, 1]\n",
    "        features = np.array([np.cos(np.pi * c * s_norm) for c in range(fourier_order + 1)])\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate: v_hat(s, w) = w^T * phi(s)\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        states = [state]\n",
    "        rewards = [0]\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                action_probs = random_policy()\n",
    "                action = np.random.choice(num_actions, p=action_probs)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    T = t + 1\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * v_hat(states[tau + n], w)\n",
    "                \n",
    "                state_tau = states[tau]\n",
    "                w += alpha * (G - v_hat(state_tau, w)) * phi(state_tau)\n",
    "            \n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a2ecb6-74cc-49be-85b7-f0e5c0d5350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def n_step_semi_gradient_td_coarse(env, num_episodes=10000, alpha=0.01, n=3, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    \n",
    "    # Define coarse coding regions (overlapping intervals)\n",
    "    num_regions = 4\n",
    "    region_size = 6  # Each region covers 6 states\n",
    "    regions = [(i, i + region_size) for i in range(0, num_states, 4)]  # e.g., (0,6), (4,10), (8,14), (12,18)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(num_regions)\n",
    "    \n",
    "    # Random policy\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Coarse coding features: 1 if state in region, 0 otherwise\n",
    "    def phi(state):\n",
    "        features = np.zeros(num_regions)\n",
    "        for idx, (start, end) in enumerate(regions):\n",
    "            if start <= state < min(end, num_states):\n",
    "                features[idx] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        states = [state]\n",
    "        rewards = [0]\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                action_probs = random_policy()\n",
    "                action = np.random.choice(num_actions, p=action_probs)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    T = t + 1\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * v_hat(states[tau + n], w)\n",
    "                \n",
    "                state_tau = states[tau]\n",
    "                w += alpha * (G - v_hat(state_tau, w)) * phi(state_tau)\n",
    "            \n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return w, v_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c072ed0-a9de-421a-a21c-d813b483ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def n_step_semi_gradient_td_tile(env, num_episodes=10000, alpha=0.01, n=3, gamma=1.0, num_tilings=4, tile_width=4):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    num_tiles_per_tiling = (num_states + tile_width - 1) // tile_width  # Ceiling division\n",
    "    num_features = num_tilings * num_tiles_per_tiling  # Total features\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(num_features)\n",
    "    \n",
    "    # Random policy\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Tile coding features\n",
    "    def phi(state):\n",
    "        features = np.zeros(num_features)\n",
    "        for tiling in range(num_tilings):\n",
    "            # Offset for each tiling\n",
    "            offset = tiling * (tile_width / num_tilings)\n",
    "            tile_idx = int((state + offset) // tile_width)\n",
    "            if tile_idx < num_tiles_per_tiling:\n",
    "                feature_idx = tiling * num_tiles_per_tiling + tile_idx\n",
    "                features[feature_idx] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        states = [state]\n",
    "        rewards = [0]\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                action_probs = random_policy()\n",
    "                action = np.random.choice(num_actions, p=action_probs)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    T = t + 1\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * v_hat(states[tau + n], w)\n",
    "                \n",
    "                state_tau = states[tau]\n",
    "                w += alpha * (G - v_hat(state_tau, w)) * phi(state_tau)\n",
    "            \n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c96de-9f68-427c-bb88-30965811fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Neural Network for Q-value approximation\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        # Simple network: two linear layers\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Input: state features, Hidden: 64 units\n",
    "        self.fc2 = nn.Linear(64, output_size)  # Output: Q-values for each action\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)  # Experience replay buffer\n",
    "        self.gamma = 1.0  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Main and target networks\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        # Sample a batch from memory\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        # Compute Q-values\n",
    "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
    "        target = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss and optimize\n",
    "        loss = nn.MSELoss()(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "def main():\n",
    "    # Environment setup\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    state_size = env.observation_space.n  # 16 states\n",
    "    action_size = env.action_space.n  # 4 actions\n",
    "\n",
    "    # One-hot encoding for states\n",
    "    def one_hot(state, size):\n",
    "        return np.eye(size)[state]\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    episodes = 1000\n",
    "    target_update_freq = 10  # Update target network every 10 episodes\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot(state, state_size)\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(100):  # Max steps per episode\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state_encoded = one_hot(next_state, state_size)\n",
    "\n",
    "            # Modify reward for better learning\n",
    "            reward = 1.0 if terminated and reward > 0 else -0.01 if not done else -1.0\n",
    "            agent.remember(state, action, reward, next_state_encoded, done)\n",
    "\n",
    "            state = next_state_encoded\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            agent.replay()\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "    # Evaluate the learned policy\n",
    "    state, _ = env.reset()\n",
    "    state = one_hot(state, state_size)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done and steps < 100:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        state = one_hot(next_state, state_size)\n",
    "        steps += 1\n",
    "        print(f\"Step {steps}: State {next_state}, Action {action}, Reward {reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1658c64-2494-4663-9910-f5afa849fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstd(env,num_episodes=10000,epsilon=0.1,gamma=0.99):\n",
    "    num_actions=env.action_space.n\n",
    "    num_states=env.observation_space.n\n",
    "    d=num_states\n",
    "    A_inv=np.eye(d)/epsilon\n",
    "    b=np.zeros(d)\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions)/num_actions\n",
    "    def phi(state):\n",
    "        features=np.zeros(d)\n",
    "        features[state]=1.0\n",
    "        return features\n",
    "    def v_hat(state,w):\n",
    "        return np.dot(w,phi(state))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        states,_=env.reset()\n",
    "        x=phi(state)\n",
    "        while True:\n",
    "            action_prob=random_policy()\n",
    "            action=np.random.choice(num_actions,p=action_prob)\n",
    "            next_state,reward,terminated,truncated,_=env.step(action)\n",
    "            x_next=phi(next_state)\n",
    "            v=np.dot(A_inv.T,(x-gamma*x_next))\n",
    "            A_inv=A_inv-np.outer(np.dot(A_inv,x),v.T)/(1 + np.dot(v.T, x))\n",
    "            b=b+reward*x\n",
    "            w=np.dot(A_inv,b)\n",
    "            state = next_state\n",
    "            x = x_next\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    \n",
    "    return w, v_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d0628c-7630-4b4a-bc59-a327d87c209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_based_nearest_neighbor(env, num_episodes=10000, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    \n",
    "    # Memory to store (state, value) pairs\n",
    "    memory = []  # List of tuples (state, value)\n",
    "    \n",
    "    # Random policy\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Simulate episodes to collect training examples using Monte Carlo\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        trajectory = [(state, 0)]  # (state, reward)\n",
    "        \n",
    "        # Generate a trajectory\n",
    "        while True:\n",
    "            action_probs = random_policy()\n",
    "            action = np.random.choice(num_actions, p=action_probs)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            trajectory.append((next_state, reward))\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Compute returns (Monte Carlo) for each state in the trajectory\n",
    "        G = 0\n",
    "        for t in range(len(trajectory) - 2, -1, -1):\n",
    "            state, reward = trajectory[t]\n",
    "            G = reward + gamma * G\n",
    "            # Store or update the (state, value) pair in memory\n",
    "            # If state already exists, update its value (average for simplicity)\n",
    "            found = False\n",
    "            for i, (s, v) in enumerate(memory):\n",
    "                if s == state:\n",
    "                    memory[i] = (s, (v + G) / 2)  # Average the values\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                memory.append((state, G))\n",
    "    \n",
    "    # Nearest neighbor value function estimate\n",
    "    def v_hat(query_state):\n",
    "        if not memory:\n",
    "            return 0.0  # Default value if memory is empty\n",
    "        \n",
    "        # Find the nearest state in memory\n",
    "        # For FrozenLake, use absolute difference as distance\n",
    "        distances = [abs(query_state - s) for s, _ in memory]\n",
    "        nearest_idx = np.argmin(distances)\n",
    "        _, value = memory[nearest_idx]\n",
    "        return value\n",
    "    \n",
    "    return memory, v_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2eec2b-86f7-4fc9-8629-eb645a9becd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def n_step_semi_gradient_td_with_interest_emphasis(env, num_episodes=10000, alpha=0.01, n=3, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    d = num_states  # Feature dimension (one-hot encoding)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(d)\n",
    "    \n",
    "    # Random policy\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Feature representation: one-hot encoding\n",
    "    def phi(state):\n",
    "        features = np.zeros(d)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Interest function: higher interest for states closer to the goal (state 15)\n",
    "    def interest(state):\n",
    "        distance_to_goal = abs(state - 15)  # Goal is state 15 in FrozenLake\n",
    "        max_distance = 15  # Maximum possible distance in 4x4 grid\n",
    "        return 1.0 - (distance_to_goal / max_distance)  # Linearly decreasing interest\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        states = [state]\n",
    "        rewards = [0]\n",
    "        interests = [interest(state)]  # Store interest for each state\n",
    "        emphases = [0] * n  # Store past n emphases for M_t computation\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                action_probs = random_policy()\n",
    "                action = np.random.choice(num_actions, p=action_probs)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                interests.append(interest(next_state))\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    T = t + 1\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                # Compute emphasis M_t\n",
    "                I_t = interests[tau]\n",
    "                M_t = I_t\n",
    "                if tau - n >= 0:\n",
    "                    M_t += (gamma ** n) * emphases[tau % n]  # M_t = I_t + gamma^n * M_{t-n}\n",
    "                emphases[tau % n] = M_t\n",
    "                \n",
    "                # Compute n-step return G\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * v_hat(states[tau + n], w)\n",
    "                \n",
    "                # Update weights with emphasis\n",
    "                state_tau = states[tau]\n",
    "                grad_v = phi(state_tau)  # Gradient of v_hat is just phi for linear approximation\n",
    "                w += alpha * M_t * (G - v_hat(state_tau, w)) * grad_v\n",
    "            \n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e58fe6-c82b-4ddd-8505-d3fdf2f4e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def kernel_regression(env, num_episodes=10000, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    \n",
    "    # Memory to store (state, value) pairs\n",
    "    memory = []  # List of tuples (state, value)\n",
    "    \n",
    "    # Random policy\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Kernel function: inverse distance for discrete states\n",
    "    def kernel(s, s_prime):\n",
    "        distance = abs(s - s_prime)\n",
    "        return 1.0 / (1.0 + distance)  # Simple kernel: 1 / (1 + |s - s'|)\n",
    "\n",
    "    # Simulate episodes to collect training examples using Monte Carlo\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        trajectory = [(state, 0)]  # (state, reward)\n",
    "        \n",
    "        # Generate a trajectory\n",
    "        while True:\n",
    "            action_probs = random_policy()\n",
    "            action = np.random.choice(num_actions, p=action_probs)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            trajectory.append((next_state, reward))\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Compute returns (Monte Carlo) for each state in the trajectory\n",
    "        G = 0\n",
    "        for t in range(len(trajectory) - 2, -1, -1):\n",
    "            state, reward = trajectory[t]\n",
    "            G = reward + gamma * G\n",
    "            # Store or update the (state, value) pair in memory\n",
    "            found = False\n",
    "            for i, (s, v) in enumerate(memory):\n",
    "                if s == state:\n",
    "                    memory[i] = (s, (v + G) / 2)  # Average the values\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                memory.append((state, G))\n",
    "    \n",
    "    # Kernel regression value function estimate\n",
    "    def v_hat(state_to_query):  # Renamed parameter for clarity\n",
    "        if not memory:\n",
    "            return 0.0  # Default value if memory is empty\n",
    "        \n",
    "        # Compute weighted average using kernel function\n",
    "        total_weight = 0.0\n",
    "        weighted_sum = 0.0\n",
    "        for stored_state, value in memory:\n",
    "            weight = kernel(state_to_query, stored_state)\n",
    "            weighted_sum += weight * value\n",
    "            total_weight += weight\n",
    "        \n",
    "        if total_weight == 0:\n",
    "            return 0.0  # Avoid division by zero\n",
    "        return weighted_sum / total_weight  # Normalize by total weight\n",
    "    \n",
    "    return memory, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85cab2d3-da1e-49d6-8d03-9024df93d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def n_step_semi_gradient_td_with_interest_emphasis(env, num_episodes=10000, alpha=0.01, n=3, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    d = num_states  # Feature dimension (one-hot encoding)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(d)\n",
    "    \n",
    "    # Random policy\n",
    "    def random_policy():\n",
    "        return np.ones(num_actions) / num_actions\n",
    "    \n",
    "    # Feature representation: one-hot encoding\n",
    "    def phi(state):\n",
    "        features = np.zeros(d)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Value function estimate\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "    \n",
    "    # Interest function: higher interest for states closer to the goal (state 15)\n",
    "    def interest(state):\n",
    "        distance_to_goal = abs(state - 15)  # Goal is state 15 in FrozenLake\n",
    "        max_distance = 15  # Maximum possible distance in 4x4 grid\n",
    "        return 1.0 - (distance_to_goal / max_distance)  # Linearly decreasing interest\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        states = [state]\n",
    "        rewards = [0]\n",
    "        interests = [interest(state)]  # Store interest for each state\n",
    "        emphases = [0] * n  # Store past n emphases for M_t computation\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                action_probs = random_policy()\n",
    "                action = np.random.choice(num_actions, p=action_probs)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                states.append(next_state)\n",
    "                rewards.append(reward)\n",
    "                interests.append(interest(next_state))\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    T = t + 1\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                # Compute emphasis M_t\n",
    "                I_t = interests[tau]\n",
    "                M_t = I_t\n",
    "                if tau - n >= 0:\n",
    "                    M_t += (gamma ** n) * emphases[tau % n]  # M_t = I_t + gamma^n * M_{t-n}\n",
    "                emphases[tau % n] = M_t\n",
    "                \n",
    "                # Compute n-step return G\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i]\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * v_hat(states[tau + n], w)\n",
    "                \n",
    "                # Update weights with emphasis\n",
    "                state_tau = states[tau]\n",
    "                grad_v = phi(state_tau)  # Gradient of v_hat is just phi for linear approximation\n",
    "                w += alpha * M_t * (G - v_hat(state_tau, w)) * grad_v\n",
    "            \n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74f578-c694-4b34-92b8-27d127beea96",
   "metadata": {},
   "source": [
    "# chapter 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e752f8-4312-4a97-b040-bf89bb6bd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def semi_gradient_sarsa(env, num_episodes=10000, alpha=0.01, gamma=1.0, epsilon=0.1):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    d = num_states * num_actions  # Feature dimension for state-action pairs\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(d)\n",
    "    \n",
    "    # Feature representation: one-hot encoding for state-action pair\n",
    "    def phi(state, action):\n",
    "        features = np.zeros(d)\n",
    "        index = state * num_actions + action  # Unique index for (state, action)\n",
    "        features[index] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Action-value function estimate\n",
    "    def q_hat(state, action, w):\n",
    "        return np.dot(w, phi(state, action))\n",
    "    \n",
    "    # Epsilon-greedy policy\n",
    "    def epsilon_greedy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)  # Explore\n",
    "        else:\n",
    "            q_values = [q_hat(state, a, w) for a in range(num_actions)]\n",
    "            return np.argmax(q_values)  # Exploit\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = epsilon_greedy(state, w)  # Initial action (ε-greedy)\n",
    "        \n",
    "        while True:\n",
    "            # Take action, observe R, S'\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                # Update for terminal state: target is just the reward\n",
    "                grad_q = phi(state, action)\n",
    "                w += alpha * (reward - q_hat(state, action, w)) * grad_q\n",
    "                break\n",
    "            \n",
    "            # Choose A' using ε-greedy policy\n",
    "            next_action = epsilon_greedy(next_state, w)\n",
    "            \n",
    "            # Update weights\n",
    "            target = reward + gamma * q_hat(next_state, next_action, w)\n",
    "            grad_q = phi(state, action)\n",
    "            w += alpha * (target - q_hat(state, action, w)) * grad_q\n",
    "            \n",
    "            # Update state and action\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    \n",
    "    return w, q_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2608901-ac11-4001-b466-f8ff8a86b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def n_step_semi_gradient_sarsa(env, num_episodes=10000, alpha=0.01, gamma=1.0, epsilon=0.1, n=3):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    d = num_states * num_actions  # Feature dimension for state-action pairs\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.zeros(d)\n",
    "    \n",
    "    # Feature representation: one-hot encoding for state-action pair\n",
    "    def phi(state, action):\n",
    "        features = np.zeros(d)\n",
    "        index = state * num_actions + action  # Unique index for (state, action)\n",
    "        features[index] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Action-value function estimate\n",
    "    def q_hat(state, action, w):\n",
    "        return np.dot(w, phi(state, action))\n",
    "    \n",
    "    # Epsilon-greedy policy\n",
    "    def epsilon_greedy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)  # Explore\n",
    "        else:\n",
    "            q_values = [q_hat(state, a, w) for a in range(num_actions)]\n",
    "            return np.argmax(q_values)  # Exploit\n",
    "    \n",
    "    # Loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = epsilon_greedy(state, w)  # Initial action A_0\n",
    "        \n",
    "        # Circular buffers for storing S_t, A_t, R_t (mod n+1)\n",
    "        buffer_size = n + 1\n",
    "        states = [0] * buffer_size  # S_t\n",
    "        actions = [0] * buffer_size  # A_t\n",
    "        rewards = [0] * buffer_size  # R_t\n",
    "        states[0] = state\n",
    "        actions[0] = action\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                # Take action A_t, observe R_{t+1}, S_{t+1}\n",
    "                next_state, reward, terminated, truncated, _ = env.step(actions[t % buffer_size])\n",
    "                states[(t + 1) % buffer_size] = next_state\n",
    "                rewards[(t + 1) % buffer_size] = reward\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Select A_{t+1} using ε-greedy\n",
    "                    next_action = epsilon_greedy(next_state, w)\n",
    "                    actions[(t + 1) % buffer_size] = next_action\n",
    "            \n",
    "            # Time to update (τ = t - n + 1)\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                # Compute n-step return G_{τ:τ+n}\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    G += (gamma ** (i - tau - 1)) * rewards[i % buffer_size]\n",
    "                if tau + n < T:\n",
    "                    G += (gamma ** n) * q_hat(states[(tau + n) % buffer_size], actions[(tau + n) % buffer_size], w)\n",
    "                \n",
    "                # Update weights\n",
    "                state_tau = states[tau % buffer_size]\n",
    "                action_tau = actions[tau % buffer_size]\n",
    "                grad_q = phi(state_tau, action_tau)\n",
    "                w += alpha * (G - q_hat(state_tau, action_tau, w)) * grad_q\n",
    "            \n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "    \n",
    "    return w, q_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996340a8-406a-4092-a8aa-10463caa3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def differential_semi_gradient_sarsa(env, num_steps=100000, alpha=0.01, beta=0.01, epsilon=0.1, gamma=1.0):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    d = num_states * num_actions  # Feature dimension for state-action pairs\n",
    "    \n",
    "    # Initialize weights and average reward\n",
    "    w = np.zeros(d)  # Value-function weights\n",
    "    avg_reward = 0.0  # R̄ (average reward estimate)\n",
    "    \n",
    "    # Feature representation: one-hot encoding for state-action pair\n",
    "    def phi(state, action):\n",
    "        features = np.zeros(d)\n",
    "        index = state * num_actions + action  # Unique index for (state, action)\n",
    "        features[index] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Action-value function estimate\n",
    "    def q_hat(state, action, w):\n",
    "        return np.dot(w, phi(state, action))\n",
    "    \n",
    "    # Epsilon-greedy policy\n",
    "    def epsilon_greedy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)  # Explore\n",
    "        else:\n",
    "            q_values = [q_hat(state, a, w) for a in range(num_actions)]\n",
    "            return np.argmax(q_values)  # Exploit\n",
    "    \n",
    "    # Initialize state and action\n",
    "    state, _ = env.reset()\n",
    "    action = epsilon_greedy(state, w)\n",
    "    \n",
    "    # Loop for each step\n",
    "    for step in range(num_steps):\n",
    "        # Take action, observe R, S'\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        # If terminal, reset the environment (treat as continuing task)\n",
    "        if terminated or truncated:\n",
    "            next_state, _ = env.reset()\n",
    "        \n",
    "        # Choose A' using ε-greedy policy\n",
    "        next_action = epsilon_greedy(next_state, w)\n",
    "        \n",
    "        # Update average reward R̄\n",
    "        delta = reward - avg_reward + q_hat(next_state, next_action, w) - q_hat(state, action, w)\n",
    "        avg_reward += beta * delta\n",
    "        \n",
    "        # Update weights\n",
    "        grad_q = phi(state, action)\n",
    "        w += alpha * delta * grad_q\n",
    "        \n",
    "        # Update state and action\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    \n",
    "    return w, q_hat, avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d6765f-f2b4-432d-8ca7-6d5002dcd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def differential_n_step_semi_gradient_sarsa(env, num_steps=100000, alpha=0.01, beta=0.01, epsilon=0.1, gamma=1.0, n=3):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    d = num_states * num_actions  # Feature dimension for state-action pairs\n",
    "    \n",
    "    # Initialize weights and average reward\n",
    "    w = np.zeros(d)  # Value-function weights\n",
    "    avg_reward = 0.0  # R̄ (average reward estimate)\n",
    "    \n",
    "    # Feature representation: one-hot encoding for state-action pair\n",
    "    def phi(state, action):\n",
    "        features = np.zeros(d)\n",
    "        index = state * num_actions + action  # Unique index for (state, action)\n",
    "        features[index] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Action-value function estimate\n",
    "    def q_hat(state, action, w):\n",
    "        return np.dot(w, phi(state, action))\n",
    "    \n",
    "    # Epsilon-greedy policy\n",
    "    def epsilon_greedy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)  # Explore\n",
    "        else:\n",
    "            q_values = [q_hat(state, a, w) for a in range(num_actions)]\n",
    "            return np.argmax(q_values)  # Exploit\n",
    "    \n",
    "    # Initialize state and action\n",
    "    state, _ = env.reset()\n",
    "    action = epsilon_greedy(state, w)\n",
    "    \n",
    "    # Circular buffers for storing S_t, A_t, R_t (mod n+1)\n",
    "    buffer_size = n + 1\n",
    "    states = [0] * buffer_size  # S_t\n",
    "    actions = [0] * buffer_size  # A_t\n",
    "    rewards = [0] * buffer_size  # R_t\n",
    "    states[0] = state\n",
    "    actions[0] = action\n",
    "    \n",
    "    # Loop for each step\n",
    "    t = 0\n",
    "    while t < num_steps:\n",
    "        # Take action A_t, observe R_{t+1}, S_{t+1}\n",
    "        next_state, reward, terminated, truncated, _ = env.step(actions[t % buffer_size])\n",
    "        \n",
    "        # If terminal, reset the environment (treat as continuing task)\n",
    "        if terminated or truncated:\n",
    "            next_state, _ = env.reset()\n",
    "        \n",
    "        states[(t + 1) % buffer_size] = next_state\n",
    "        rewards[(t + 1) % buffer_size] = reward\n",
    "        \n",
    "        # Select A_{t+1} using ε-greedy\n",
    "        next_action = epsilon_greedy(next_state, w)\n",
    "        actions[(t + 1) % buffer_size] = next_action\n",
    "        \n",
    "        # Time to update (τ = t - n + 1)\n",
    "        tau = t - n + 1\n",
    "        if tau >= 0:\n",
    "            # Compute n-step differential return\n",
    "            delta = 0\n",
    "            for i in range(tau + 1, tau + n + 1):\n",
    "                delta += rewards[i % buffer_size] - avg_reward\n",
    "            delta += q_hat(states[(tau + n) % buffer_size], actions[(tau + n) % buffer_size], w) - q_hat(states[tau % buffer_size], actions[tau % buffer_size], w)\n",
    "            \n",
    "            # Update average reward R̄\n",
    "            avg_reward += beta * delta\n",
    "            \n",
    "            # Update weights\n",
    "            grad_q = phi(states[tau % buffer_size], actions[tau % buffer_size])\n",
    "            w += alpha * delta * grad_q\n",
    "        \n",
    "        t += 1\n",
    "    \n",
    "    return w, q_hat, avg_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2660ed40-35a3-428b-97a1-de54c9a0f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward Estimate: 0.11\n",
      "Estimated Action-Value Function (Differential n-step SARSA, ε-greedy):\n",
      "Action 0: -0.36 -0.35 -0.21 -0.11\n",
      "Action 1: -0.27 -0.13 0.01 -0.10\n",
      "Action 2: -0.28 -0.14 -0.20 -0.13\n",
      "Action 3: -0.37 -0.23 -0.14 -0.15\n",
      "Action 0: -0.26 0.00 -0.09 0.00\n",
      "Action 1: -0.16 0.00 0.28 0.00\n",
      "Action 2: -0.13 0.00 -0.09 0.00\n",
      "Action 3: -0.32 0.00 -0.02 0.00\n",
      "Action 0: -0.08 -0.00 0.18 0.00\n",
      "Action 1: -0.12 0.18 0.53 0.00\n",
      "Action 2: 0.03 0.25 -0.07 0.00\n",
      "Action 3: -0.19 -0.09 0.16 0.00\n",
      "Action 0: 0.00 -0.07 0.40 0.00\n",
      "Action 1: 0.00 0.31 0.63 0.00\n",
      "Action 2: 0.00 0.53 0.85 0.00\n",
      "Action 3: 0.00 0.19 0.42 0.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def differential_n_step_semi_gradient_sarsa(env, num_steps=200000, alpha=0.05, beta=0.05, epsilon=0.3, gamma=0.9, n=3):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "    d = num_states * num_actions  # Feature dimension for state-action pairs\n",
    "    \n",
    "    # Initialize weights and average reward\n",
    "    w = np.zeros(d)  # Value-function weights\n",
    "    avg_reward = 0.0  # R̄ (average reward estimate)\n",
    "    \n",
    "    # Feature representation: one-hot encoding for state-action pair\n",
    "    def phi(state, action):\n",
    "        features = np.zeros(d)\n",
    "        index = state * num_actions + action  # Unique index for (state, action)\n",
    "        features[index] = 1.0\n",
    "        return features\n",
    "    \n",
    "    # Action-value function estimate\n",
    "    def q_hat(state, action, w):\n",
    "        return np.dot(w, phi(state, action))\n",
    "    \n",
    "    # Epsilon-greedy policy\n",
    "    def epsilon_greedy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)  # Explore\n",
    "        else:\n",
    "            q_values = [q_hat(state, a, w) for a in range(num_actions)]\n",
    "            return np.argmax(q_values)  # Exploit\n",
    "    \n",
    "    # Initialize state and action\n",
    "    state, _ = env.reset()\n",
    "    action = epsilon_greedy(state, w)\n",
    "    \n",
    "    # Circular buffers for storing S_t, A_t, R_t (mod n+1)\n",
    "    buffer_size = n + 1\n",
    "    states = [0] * buffer_size  # S_t\n",
    "    actions = [0] * buffer_size  # A_t\n",
    "    rewards = [0] * buffer_size  # R_t\n",
    "    terminated_flags = [False] * buffer_size  # Track termination\n",
    "    states[0] = state\n",
    "    actions[0] = action\n",
    "    \n",
    "    # Loop for each step\n",
    "    t = 0\n",
    "    while t < num_steps:\n",
    "        # Take action A_t, observe R_{t+1}, S_{t+1}\n",
    "        next_state, reward, terminated, truncated, _ = env.step(actions[t % buffer_size])\n",
    "        \n",
    "        # If terminal, reset the environment (treat as continuing task)\n",
    "        if terminated or truncated:\n",
    "            next_state, _ = env.reset()\n",
    "        \n",
    "        states[(t + 1) % buffer_size] = next_state\n",
    "        rewards[(t + 1) % buffer_size] = reward\n",
    "        terminated_flags[(t + 1) % buffer_size] = terminated or truncated\n",
    "        \n",
    "        # Select A_{t+1} using ε-greedy\n",
    "        next_action = epsilon_greedy(next_state, w)\n",
    "        actions[(t + 1) % buffer_size] = next_action\n",
    "        \n",
    "        # Time to update (τ = t - n + 1)\n",
    "        tau = t - n + 1\n",
    "        if tau >= 0:\n",
    "            # Compute n-step differential return\n",
    "            delta = 0\n",
    "            for i in range(tau + 1, tau + n + 1):\n",
    "                delta += gamma ** (i - tau - 1) * (rewards[i % buffer_size] - avg_reward)\n",
    "                if terminated_flags[i % buffer_size]:  # If termination within n steps, stop\n",
    "                    break\n",
    "            else:\n",
    "                # If no termination within n steps, add the q-value of the next state-action\n",
    "                delta += (gamma ** n) * q_hat(states[(tau + n) % buffer_size], actions[(tau + n) % buffer_size], w)\n",
    "            \n",
    "            delta -= q_hat(states[tau % buffer_size], actions[tau % buffer_size], w)\n",
    "            \n",
    "            # Update average reward R̄\n",
    "            avg_reward += beta * delta\n",
    "            \n",
    "            # Update weights\n",
    "            grad_q = phi(states[tau % buffer_size], actions[tau % buffer_size])\n",
    "            w += alpha * delta * grad_q\n",
    "        \n",
    "        t += 1\n",
    "    \n",
    "    return w, q_hat, avg_reward\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    w, q_hat, avg_reward = differential_n_step_semi_gradient_sarsa(env, num_steps=200000, alpha=0.05, beta=0.05, epsilon=0.3, gamma=0.9, n=3)\n",
    "    \n",
    "    print(f\"Average Reward Estimate: {avg_reward:.2f}\")\n",
    "    print(\"Estimated Action-Value Function (Differential n-step SARSA, ε-greedy):\")\n",
    "    for i in range(4):\n",
    "        for a in range(4):  # 4 actions: Left, Down, Right, Up\n",
    "            row = [f\"{q_hat(i*4 + j, a, w):.2f}\" for j in range(4)]\n",
    "            print(f\"Action {a}: {' '.join(row)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a5004-6029-45d9-82f6-353e19b395e1",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb410e0-f187-4cf6-8d10-1b016b253541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def tdc(env, num_steps=100000, alpha=0.005, beta=0.05, epsilon=0.1, gamma=0.9):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    d = num_states  # Feature dimension (one-hot encoding for states)\n",
    "\n",
    "    # Initialize weights and auxiliary vector\n",
    "    w = np.zeros(d)  # For v(s, w)\n",
    "    v = np.zeros(d)  # Auxiliary vector for gradient correction\n",
    "\n",
    "    # Feature representation: one-hot encoding for states\n",
    "    def phi(state):\n",
    "        features = np.zeros(d)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Value function estimate\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "\n",
    "    # Policies\n",
    "    def behavior_policy():\n",
    "        return np.random.randint(num_actions)  # Random policy\n",
    "\n",
    "    def target_policy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)\n",
    "        q_values = [sum(v_hat(next_state, w) for next_state, prob in env.P[state][a].items()) for a in range(num_actions)]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # Initialize state\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Loop for each step\n",
    "    for t in range(num_steps):\n",
    "        # Take action using behavior policy\n",
    "        action = behavior_policy()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # If terminal, reset environment\n",
    "        if terminated or truncated:\n",
    "            next_state, _ = env.reset()\n",
    "\n",
    "        # Compute importance sampling ratio\n",
    "        pi_a = 1.0 if action == target_policy(state, w) else 0.0  # ε-greedy target policy\n",
    "        b_a = 1.0 / num_actions  # Random behavior policy\n",
    "        rho = pi_a / b_a\n",
    "\n",
    "        # Features\n",
    "        x_t = phi(state)\n",
    "        x_t_next = phi(next_state)\n",
    "\n",
    "        # TD error\n",
    "        delta = reward + gamma * v_hat(next_state, w) - v_hat(state, w)\n",
    "\n",
    "        # Update auxiliary vector v (LMS rule)\n",
    "        v += beta * rho * (delta - np.dot(v, x_t)) * x_t\n",
    "\n",
    "        # Update weights w (TDC update)\n",
    "        w += alpha * rho * (delta * x_t - x_t_next * np.dot(x_t, v))\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329fbea4-60c6-45fc-abd2-2186c2de4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def emphatic_td(env, num_steps=100000, alpha=0.03, gamma=0.9, epsilon=0.1):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    d = num_states  # Feature dimension (one-hot encoding for states)\n",
    "\n",
    "    # Initialize weights\n",
    "    w = np.zeros(d)  # For v(s, w)\n",
    "\n",
    "    # Feature representation: one-hot encoding for states\n",
    "    def phi(state):\n",
    "        features = np.zeros(d)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Value function estimate\n",
    "    def v_hat(state, w):\n",
    "        return np.dot(w, phi(state))\n",
    "\n",
    "    # Policies\n",
    "    def behavior_policy():\n",
    "        return np.random.randint(num_actions)  # Random policy\n",
    "\n",
    "    def target_policy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)\n",
    "        q_values = [sum(v_hat(next_state, w) for next_state, prob in env.P[state][a].items()) for a in range(num_actions)]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # Initialize state and emphasis\n",
    "    state, _ = env.reset()\n",
    "    M = 0.0  # Emphasis M_t, initialized as M_{-1} = 0\n",
    "    rho_prev = 1.0  # Initial rho_{t-1} for t=0\n",
    "\n",
    "    # Loop for each step\n",
    "    for t in range(num_steps):\n",
    "        # Take action using behavior policy\n",
    "        action = behavior_policy()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # If terminal, reset environment and emphasis\n",
    "        if terminated or truncated:\n",
    "            next_state, _ = env.reset()\n",
    "            M = 0.0  # Reset emphasis on episode boundary\n",
    "            rho_prev = 1.0  # Reset rho for new episode\n",
    "\n",
    "        # Compute importance sampling ratio\n",
    "        pi_a = 1.0 if action == target_policy(state, w) else 0.0  # ε-greedy target policy\n",
    "        b_a = 1.0 / num_actions  # Random behavior policy\n",
    "        rho = pi_a / b_a\n",
    "\n",
    "        # Features\n",
    "        x_t = phi(state)\n",
    "        x_t_next = phi(next_state)\n",
    "\n",
    "        # TD error\n",
    "        delta = reward + gamma * v_hat(next_state, w) - v_hat(state, w)\n",
    "\n",
    "        # Interest (set to 1 for simplicity, as in the book's example)\n",
    "        I_t = 1.0\n",
    "\n",
    "        # Update emphasis\n",
    "        M = gamma * rho_prev * M + I_t\n",
    "\n",
    "        # Update weights\n",
    "        w += alpha * M * rho * delta * x_t\n",
    "\n",
    "        # Update state and rho for next iteration\n",
    "        state = next_state\n",
    "        rho_prev = rho\n",
    "\n",
    "    return w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824fff2d-9a3f-43d2-ae47-1cd5e27031c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karansoni/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Total Reward: 10.0\n",
      "Episode 100, Total Reward: 27.0\n",
      "Episode 150, Total Reward: 12.0\n",
      "Episode 200, Total Reward: 19.0\n",
      "Episode 250, Total Reward: 12.0\n",
      "Episode 300, Total Reward: 44.0\n",
      "Episode 350, Total Reward: 18.0\n",
      "Episode 400, Total Reward: 31.0\n",
      "Episode 450, Total Reward: 27.0\n",
      "Episode 500, Total Reward: 30.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Define a simple neural network for value function approximation\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize model, optimizer, loss\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "model = ValueNetwork(obs_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Select a random action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Step in the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        # Compute target using TD(0)\n",
    "        with torch.no_grad():\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += gamma * model(next_state_tensor).item()\n",
    "            target = torch.tensor([target], dtype=torch.float32)\n",
    "\n",
    "        # Predict current value\n",
    "        value = model(state)\n",
    "\n",
    "        # Compute loss and update\n",
    "        loss = loss_fn(value, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state_tensor\n",
    "        total_reward += reward\n",
    "\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b36a9-949f-4948-bd80-67f7ad0755fc",
   "metadata": {},
   "source": [
    "# chapter 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc568cd-d290-4e77-967c-a81eee4a8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_lambda_return(env,num_episodes=10000,alpha=0.1,gamma=0.9,lambda_=0.9,epsilon=0.1):\n",
    "    num_actions=env.action_space.n\n",
    "    num_states=env.observation_space.n\n",
    "    d=num_states\n",
    "    w=np.zeros(d)\n",
    "\n",
    "    def phi(state):\n",
    "        features=np.zeros(d)\n",
    "        features[state]=1\n",
    "        return features\n",
    "    def v_hat(state,w):\n",
    "        return np.dot(w,phi(state))\n",
    "    def epsilon_greedy(state,w):\n",
    "        if np.random.rand()<epsilon:\n",
    "            return np.random.randit(num_actions)\n",
    "        q_values=[sum(v_hat(next_state,w) for next_state,p in env.P[state][a].items()) for a in range(num_actions)]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        trajectory=[]\n",
    "        state,_=env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            action=epsilon_greedy(state,w)\n",
    "            next_state,reward,terminated,trucated,_=env.step(action)\n",
    "            trajectory.append((state,action,reward,next_state))\n",
    "            state=next_state\n",
    "            done=termminated or truncated\n",
    "        T=len(trajectory)\n",
    "        G_lambda=np.zeros(T)\n",
    "        G=np.zeros(T)\n",
    "        for t in range(T-1,-1,-1):\n",
    "            _,_,reward,next_state=trajectory[t]\n",
    "            if t==(T-1):\n",
    "                G[t]=reward\n",
    "            else:\n",
    "                G[t]=reward+gamma*G[t+1]\n",
    "        for t in range(t):\n",
    "            sum_n_step=0.0\n",
    "            for n in range(1,T-t):\n",
    "                G_t_to_tn=0.0\n",
    "                for i in range(t,t+n):\n",
    "                    _,_,reward,_=trajectory[i]\n",
    "                    G_t_to_tn += (gamma ** (i-t)) * reward\n",
    "                if t+n < T:\n",
    "                    _, _, _, next_state = trajectory[t+n-1]\n",
    "                    G_t_to_tn += (gamma ** n) * v_hat(next_state, w)\n",
    "                sum_n_step += (lambda_ ** (n-1)) * G_t_to_tn\n",
    "\n",
    "            # Add the Monte Carlo term for n ≥ T-t\n",
    "            G_lambda[t] = (1 - lambda_) * sum_n_step + (lambda_ ** (T-t-1)) * G[t]\n",
    "\n",
    "        # Update weights for each t using the λ-return\n",
    "        for t in range(T):\n",
    "            state, _, _, _ = trajectory[t]\n",
    "            x_t = phi(state)\n",
    "            w += alpha * (G_lambda[t] - v_hat(state, w)) * x_t\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309eeda5-2e4c-4041-9cd1-dccb4f9cccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_gradient_td_lambda(env,num_episodes=10000,alpha=0.1,gamma=0.9,lamnda=0.1,epsilon=0.1):\n",
    "    num_actions=env.observation_space.n\n",
    "    num_states=env.action_space.n\n",
    "    d=num_states\n",
    "    w=np.zeros(d)\n",
    "    def phi(state):\n",
    "        features=np.zeros(d)\n",
    "        features[state]=1.0\n",
    "        return features\n",
    "    def v_hat(state, w):\n",
    "        # Ensure terminal states have value 0\n",
    "        if state == 15:  # Terminal state in FrozenLake (goal state)\n",
    "            return 0.0\n",
    "        return np.dot(w, phi(state))\n",
    "    def epsilon_greedy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)\n",
    "        q_values = [sum(v_hat(next_state, w) for next_state, prob in env.P[state][a].items()) for a in range(num_actions)]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # Main loop over episodes\n",
    "    for episode in range(num_action):\n",
    "        state,_=env.reset()\n",
    "        z=np.zeros(d)\n",
    "        done=False\n",
    "        while not done:\n",
    "            action=epsilon_greedy(state,w)\n",
    "            next_state,reward,terminated,truncated,_=env.step(action)\n",
    "            done=terminated or truncated\n",
    "            x_t=phi(state)\n",
    "            x_t_next=phi(next_state)\n",
    "            z = gamma * lambda_ * z + x_t  # For linear FA, ∇v̂(S, w) = x(S)\n",
    "\n",
    "            # Compute TD error: δ = R + γv̂(S', w) - v̂(S, w)\n",
    "            delta = reward + gamma * v_hat(next_state, w) - v_hat(state, w)\n",
    "\n",
    "            # Update weights: w ← w + αδz\n",
    "            w += alpha * delta * z\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "    return w, v_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfda5902-2606-4f97-9639-e3293b90d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_online_td_lambda(env,num_episodes=10000,alpha=0.1,gamma=0.0,epsilon=0.1,lambda_=0.1):\n",
    "    num_actions=env.observation_space.n\n",
    "    num_states=env.action_space.n\n",
    "    d=num_states\n",
    "    w=np.zeros(d)\n",
    "    def x(state):\n",
    "        # Ensure terminal states have feature vector 0\n",
    "        if state == 15:  # Terminal state in FrozenLake (goal state)\n",
    "            return np.zeros(d)\n",
    "        features = np.zeros(d)\n",
    "        features[state] = 1.0\n",
    "        return features\n",
    "    def v_hat(state,w):\n",
    "        return np.dot(w,x(state))\n",
    "    def epsilon_greedy(state, w):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(num_actions)\n",
    "        q_values = [sum(v_hat(next_state, w) for next_state, prob in env.P[state][a].items()) for a in range(num_actions)]\n",
    "        return np.argmax(q_values)\n",
    "    for episode in range(num_episodes):\n",
    "        state,_=env.reset()\n",
    "        x_t=x(state)\n",
    "        z=np.zeros(d)\n",
    "        V_old=0.0 # Temporary scalar\n",
    "        done=False\n",
    "        while not done:\n",
    "            action= epsilon_greedy(state,w)\n",
    "            next_state,reward,terminated,truncated,_=env.step(action)\n",
    "            done=terminated or truncated\n",
    "            x_t_next=x(next_state)\n",
    "            # Compute values and TD error\n",
    "            V = np.dot(w, x_t)  # V = w^T x\n",
    "            V_next = np.dot(w, x_t_next)  # V' = w^T x'\n",
    "            delta = reward + gamma * V_next - V  # δ = R + γV' - V\n",
    "\n",
    "            # Update eligibility trace: z ← γλz + (1 - α z^T x) x\n",
    "            z = gamma * lambda_ * z + (1 - alpha * np.dot(z, x_t)) * x_t\n",
    "\n",
    "            # Update weights: w ← w + α(δ + V - V_old)z - α(V - V_old)x\n",
    "            w += alpha * (delta + V - V_old) * z - alpha * (V - V_old) * x_t\n",
    "\n",
    "            # Update V_old and x\n",
    "            V_old = V_next\n",
    "            x_t = x_t_next\n",
    "            state = next_state\n",
    "\n",
    "            # Check for terminal state (x' = 0)\n",
    "            if np.all(x_t == 0):\n",
    "                break\n",
    "\n",
    "    return w, v_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc79dfcb-f022-4ee8-8b41-e0e167c278ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda(env,num_episodes=10000,alpha=0.1,gamma=0.9,epsilon=0.1,lambda_=0.9):\n",
    "    num_states=env.observation_space.n\n",
    "    num_actions=env.oservation_space.n\n",
    "    d=num_actions*num_states\n",
    "    w=np.zeros(d)\n",
    "    z=np.zeros(d)\n",
    "    def F(s,a):\n",
    "        return [s*num_actions+a]\n",
    "    def q_hat(s,a,w):\n",
    "        active_features=F(s,a)\n",
    "        return sum(w[i] for i in active_features)\n",
    "    def epsilon_greedy(state,w):\n",
    "        if np.random.rand()<epsilon:\n",
    "            return np.random.randit(num_actions)\n",
    "        q_values=[q_hat(state,a,w) for a in range(num_actions)]\n",
    "        return np.argmax(q_values)\n",
    "    for episode in range(num_episodes):\n",
    "        state,_=env.reset()\n",
    "        action=epsilon_greedy(state,w)\n",
    "        z=np.zeros(d)\n",
    "        done=False\n",
    "        while not done:\n",
    "            next_state,reward,terminated,truncated,_=env.step(action)\n",
    "            done=terminated or truncated\n",
    "            q_current=q_hat(state,action,w)\n",
    "            if done:\n",
    "                delta=reward-q_current\n",
    "                next_action=None\n",
    "            else:\n",
    "                next_action=epsilon_greedy(next_state,w)\n",
    "                q_next=q_hat(next_state,next_action,w)\n",
    "                delta=reward+gamma*q_next-q_current\n",
    "            for i in F(state, action):\n",
    "                z[i] = gamma * lambda_ * z[i] + 1  # Accumulating traces\n",
    "\n",
    "            # Update weights\n",
    "            w += alpha * delta * z\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Decay eligibility trace\n",
    "            z = gamma * lambda_ * z\n",
    "\n",
    "            # Update state and action\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    return w, q_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da5619-2273-40c4-a4ef-3debfa98cb3e",
   "metadata": {},
   "source": [
    "# chapter 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65015ab-38fb-46de-827e-d366a66c6f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env,num_episodes=1000,alpha=0.1,gamma=0.9):\n",
    "    num_states=np.observation_space.n\n",
    "    num_actions=np.action_space.n\n",
    "    d=num_states*num_actions\n",
    "    theta=np.zeros(d)\n",
    "    # Feature function φ(s, a): one-hot encoding for state-action pairs\n",
    "    def phi(s, a):\n",
    "        features = np.zeros(d)\n",
    "        features[s * num_actions + a] = 1.0\n",
    "        return features\n",
    "    # Softmax policy π(a|s, θ)\n",
    "    def policy(s, theta):\n",
    "        scores = np.array([np.dot(theta, phi(s, a)) for a in range(num_actions)])\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        probs = exp_scores / np.sum(exp_scores)\n",
    "        return probs\n",
    "    def sample_action(s,theta):\n",
    "        probs=policy(s,theta)\n",
    "        return np.random.choice(num_actions,p=probs)\n",
    "    # Gradient of log π(a|s, θ)\n",
    "    def grad_log_pi(s, a, theta):\n",
    "        # ∇_θ ln π(a|s, θ) = φ(s, a) - Σ_b π(b|s, θ) φ(s, b)\n",
    "        x_sa=phi(s,a)\n",
    "        probs=policy(s,theta)\n",
    "        expected_phi=np.zeros(d)\n",
    "        for b in range(num_actions):\n",
    "            expected_phi+=probs[b]*phi(s,a)\n",
    "        return x_sa-expected_phi\n",
    "    for episode in range(num_episodes):\n",
    "        trajectory=[]\n",
    "        state,_=env.reset()\n",
    "        done=False\n",
    "        while not done:\n",
    "            action=sample_actions(s,theta)\n",
    "            next_state,reward,terminated,truncated,_=env.step(action)\n",
    "            trajectory.append((state,action,reward))\n",
    "            state=next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Compute returns and update θ\n",
    "        T = len(trajectory)\n",
    "        G = 0\n",
    "        for t in range(T-1, -1, -1):\n",
    "            state, action, reward = trajectory[t]\n",
    "            G = reward + gamma * G  # Compute return G_t\n",
    "            # Update θ: θ ← θ + α G_t ∇_θ ln π(A_t|S_t, θ)\n",
    "            grad = grad_log_pi(state, action, theta)\n",
    "            theta += alpha * G * grad\n",
    "\n",
    "    return theta, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9917b334-9a49-4b96-8c93-d82e33eb7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_with_baseline(env, num_episodes=5000, alpha_theta=0.01, alpha_w=0.1, gamma=0.9):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    d_policy = num_states * num_actions  # Feature dimension for policy (state-action pairs)\n",
    "    d_value = num_states  # Feature dimension for value function (states)\n",
    "\n",
    "    # Initialize policy and value function parameters\n",
    "    theta = np.zeros(d_policy)  # For π(a|s, θ)\n",
    "    w = np.zeros(d_value)  # For v(s, w)\n",
    "\n",
    "    # Feature function φ(s, a) for policy: one-hot encoding for state-action pairs\n",
    "    def phi(s, a):\n",
    "        features = np.zeros(d_policy)\n",
    "        features[s * num_actions + a] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Feature function x(s) for value function: one-hot encoding for states\n",
    "    def x(s):\n",
    "        features = np.zeros(d_value)\n",
    "        features[s] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Softmax policy π(a|s, θ)\n",
    "    def policy(s, theta):\n",
    "        scores = np.array([np.dot(theta, phi(s, a)) for a in range(num_actions)])\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        probs = exp_scores / np.sum(exp_scores)\n",
    "        return probs\n",
    "\n",
    "    # Sample an action from the policy\n",
    "    def sample_action(s, theta):\n",
    "        probs = policy(s, theta)\n",
    "        return np.random.choice(num_actions, p=probs)\n",
    "\n",
    "    # Gradient of log π(a|s, θ)\n",
    "    def grad_log_pi(s, a, theta):\n",
    "        x_sa = phi(s, a)\n",
    "        probs = policy(s, theta)\n",
    "        expected_phi = np.zeros(d_policy)\n",
    "        for b in range(num_actions):\n",
    "            expected_phi += probs[b] * phi(s, b)\n",
    "        return x_sa - expected_phi\n",
    "\n",
    "    # State-value function v(s, w)\n",
    "    def v_hat(s, w):\n",
    "        return np.dot(w, x(s))\n",
    "\n",
    "    # Main loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate an episode\n",
    "        trajectory = []\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = sample_action(state, theta)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            trajectory.append((state, action, reward))\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Update parameters for each step\n",
    "        T = len(trajectory)\n",
    "        G = 0\n",
    "        for t in range(T-1, -1, -1):\n",
    "            state, action, reward = trajectory[t]\n",
    "            G = reward + gamma * G  # Compute return G_t\n",
    "\n",
    "            # Compute advantage: δ = G_t - v̂(S_t, w)\n",
    "            delta = G - v_hat(state, w)\n",
    "\n",
    "            # Update value function: w ← w + α_w δ ∇v̂(S_t, w)\n",
    "            w += alpha_w * delta * x(state)\n",
    "\n",
    "            # Update policy: θ ← θ + α_θ δ ∇ln π(A_t|S_t, θ)\n",
    "            grad = grad_log_pi(state, action, theta)\n",
    "            theta += alpha_theta * delta * grad\n",
    "\n",
    "    return theta, policy, w, v_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3934db46-c378-42b1-8875-2254b33b95d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy (action probabilities):\n",
      "State 0: Action 0: 0.005 Action 1: 0.990 Action 2: 0.001 Action 3: 0.004 \n",
      "State 1: Action 0: 0.390 Action 1: 0.126 Action 2: 0.271 Action 3: 0.212 \n",
      "State 2: Action 0: 0.212 Action 1: 0.372 Action 2: 0.187 Action 3: 0.229 \n",
      "State 3: Action 0: 0.268 Action 1: 0.236 Action 2: 0.248 Action 3: 0.248 \n",
      "State 4: Action 0: 0.004 Action 1: 0.993 Action 2: 0.001 Action 3: 0.002 \n",
      "State 5: Action 0: 0.250 Action 1: 0.250 Action 2: 0.250 Action 3: 0.250 \n",
      "State 6: Action 0: 0.152 Action 1: 0.520 Action 2: 0.151 Action 3: 0.177 \n",
      "State 7: Action 0: 0.250 Action 1: 0.250 Action 2: 0.250 Action 3: 0.250 \n",
      "State 8: Action 0: 0.004 Action 1: 0.001 Action 2: 0.993 Action 3: 0.002 \n",
      "State 9: Action 0: 0.003 Action 1: 0.962 Action 2: 0.034 Action 3: 0.001 \n",
      "State 10: Action 0: 0.014 Action 1: 0.973 Action 2: 0.005 Action 3: 0.008 \n",
      "State 11: Action 0: 0.250 Action 1: 0.250 Action 2: 0.250 Action 3: 0.250 \n",
      "State 12: Action 0: 0.250 Action 1: 0.250 Action 2: 0.250 Action 3: 0.250 \n",
      "State 13: Action 0: 0.001 Action 1: 0.005 Action 2: 0.992 Action 3: 0.003 \n",
      "State 14: Action 0: 0.003 Action 1: 0.005 Action 2: 0.990 Action 3: 0.003 \n",
      "State 15: Action 0: 0.250 Action 1: 0.250 Action 2: 0.250 Action 3: 0.250 \n",
      "\n",
      "Learned State-Value Function:\n",
      "Row 0: 0.59 0.35 0.42 0.14\n",
      "Row 1: 0.65 0.00 0.62 0.00\n",
      "Row 2: 0.73 0.81 0.89 0.00\n",
      "Row 3: 0.00 0.90 1.00 0.00\n",
      "\n",
      "Average reward over 100 test episodes: 1.00\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def one_step_actor_critic(env, num_episodes=1000, alpha_theta=0.01, alpha_w=0.1, gamma=0.9):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    d_policy = num_states * num_actions  # Feature dimension for policy (state-action pairs)\n",
    "    d_value = num_states  # Feature dimension for value function (states)\n",
    "\n",
    "    # Initialize policy and value function parameters\n",
    "    theta = np.zeros(d_policy)  # For π(a|s, θ)\n",
    "    w = np.zeros(d_value)  # For v(s, w)\n",
    "\n",
    "    # Feature function φ(s, a) for policy: one-hot encoding for state-action pairs\n",
    "    def phi(s, a):\n",
    "        features = np.zeros(d_policy)\n",
    "        features[s * num_actions + a] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Feature function x(s) for value function: one-hot encoding for states\n",
    "    def x(s):\n",
    "        features = np.zeros(d_value)\n",
    "        features[s] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Softmax policy π(a|s, θ)\n",
    "    def policy(s, theta):\n",
    "        scores = np.array([np.dot(theta, phi(s, a)) for a in range(num_actions)])\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        probs = exp_scores / np.sum(exp_scores)\n",
    "        return probs\n",
    "\n",
    "    # Sample an action from the policy\n",
    "    def sample_action(s, theta):\n",
    "        probs = policy(s, theta)\n",
    "        return np.random.choice(num_actions, p=probs)\n",
    "\n",
    "    # Gradient of log π(a|s, θ)\n",
    "    def grad_log_pi(s, a, theta):\n",
    "        x_sa = phi(s, a)\n",
    "        probs = policy(s, theta)\n",
    "        expected_phi = np.zeros(d_policy)\n",
    "        for b in range(num_actions):\n",
    "            expected_phi += probs[b] * phi(s, b)\n",
    "        return x_sa - expected_phi\n",
    "\n",
    "    # State-value function v(s, w)\n",
    "    def v_hat(s, w):\n",
    "        return np.dot(w, x(s))\n",
    "\n",
    "    # Main loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize state and I\n",
    "        state, _ = env.reset()\n",
    "        I = 1.0  # Discounting factor for policy updates\n",
    "\n",
    "        # Loop until episode terminates\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Sample action\n",
    "            action = sample_action(state, theta)\n",
    "\n",
    "            # Take action, observe reward and next state\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Compute TD error: δ = R + γ v̂(S', w) - v̂(S, w)\n",
    "            v_current = v_hat(state, w)\n",
    "            v_next = 0.0 if done else v_hat(next_state, w)  # v̂(S', w) = 0 if terminal\n",
    "            delta = reward + gamma * v_next - v_current\n",
    "\n",
    "            # Update critic: w ← w + α_w δ ∇v̂(S, w)\n",
    "            w += alpha_w * delta * x(state)\n",
    "\n",
    "            # Update actor: θ ← θ + α_θ I δ ∇ln π(A|S, θ)\n",
    "            grad = grad_log_pi(state, action, theta)\n",
    "            theta += alpha_theta * I * delta * grad\n",
    "\n",
    "            # Update I and state\n",
    "            I *= gamma\n",
    "            state = next_state\n",
    "\n",
    "    return theta, policy, w, v_hat\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "    # Unwrap the environment to access the underlying FrozenLake environment\n",
    "    env = env.unwrapped\n",
    "    # Fix terminal state transitions for state 15\n",
    "    env.P[15] = {a: {(15, 0.0, True, False): 1.0} for a in range(4)}\n",
    "\n",
    "    theta, policy, w, v_hat = one_step_actor_critic(env, num_episodes=100000, alpha_theta=0.01, alpha_w=0.1, gamma=0.9)\n",
    "\n",
    "    # Print the learned policy\n",
    "    print(\"Learned Policy (action probabilities):\")\n",
    "    for s in range(16):\n",
    "        probs = policy(s, theta)\n",
    "        print(f\"State {s}: \", end=\"\")\n",
    "        for a in range(4):\n",
    "            print(f\"Action {a}: {probs[a]:.3f} \", end=\"\")\n",
    "        print()\n",
    "\n",
    "    # Print the learned state-value function\n",
    "    print(\"\\nLearned State-Value Function:\")\n",
    "    for i in range(4):\n",
    "        row = [f\"{v_hat(i*4 + j, w):.2f}\" for j in range(4)]\n",
    "        print(f\"Row {i}: {' '.join(row)}\")\n",
    "\n",
    "    # Test the policy\n",
    "    total_reward = 0\n",
    "    num_test_episodes = 100\n",
    "    for _ in range(num_test_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = np.argmax(policy(state, theta))  # Greedy action for testing\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            done = terminated or truncated\n",
    "        total_reward += episode_reward\n",
    "    print(f\"\\nAverage reward over {num_test_episodes} test episodes: {total_reward / num_test_episodes:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffdf7161-0cdd-4af4-bc82-0fe0299e6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def actor_critic_with_traces(env, num_episodes=1000, alpha_theta=0.01, alpha_w=0.1, gamma=0.9, lambda_theta=0.8, lambda_w=0.8):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    d_policy = num_states * num_actions  # Feature dimension for policy (state-action pairs)\n",
    "    d_value = num_states  # Feature dimension for value function (states)\n",
    "\n",
    "    # Initialize policy and value function parameters\n",
    "    theta = np.zeros(d_policy)  # For π(a|s, θ)\n",
    "    w = np.zeros(d_value)  # For v(s, w)\n",
    "\n",
    "    # Feature function φ(s, a) for policy: one-hot encoding for state-action pairs\n",
    "    def phi(s, a):\n",
    "        features = np.zeros(d_policy)\n",
    "        features[s * num_actions + a] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Feature function x(s) for value function: one-hot encoding for states\n",
    "    def x(s):\n",
    "        features = np.zeros(d_value)\n",
    "        features[s] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Softmax policy π(a|s, θ)\n",
    "    def policy(s, theta):\n",
    "        scores = np.array([np.dot(theta, phi(s, a)) for a in range(num_actions)])\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        probs = exp_scores / np.sum(exp_scores)\n",
    "        return probs\n",
    "\n",
    "    # Sample an action from the policy\n",
    "    def sample_action(s, theta):\n",
    "        probs = policy(s, theta)\n",
    "        return np.random.choice(num_actions, p=probs)\n",
    "\n",
    "    # Gradient of log π(a|s, θ)\n",
    "    def grad_log_pi(s, a, theta):\n",
    "        x_sa = phi(s, a)\n",
    "        probs = policy(s, theta)\n",
    "        expected_phi = np.zeros(d_policy)\n",
    "        for b in range(num_actions):\n",
    "            expected_phi += probs[b] * phi(s, b)\n",
    "        return x_sa - expected_phi\n",
    "\n",
    "    # State-value function v(s, w)\n",
    "    def v_hat(s, w):\n",
    "        return np.dot(w, x(s))\n",
    "\n",
    "    # Main loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize state and eligibility traces\n",
    "        state, _ = env.reset()\n",
    "        z_theta = np.zeros(d_policy)  # Eligibility trace for θ\n",
    "        z_w = np.zeros(d_value)  # Eligibility trace for w\n",
    "\n",
    "        # Loop until episode terminates\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Sample action\n",
    "            action = sample_action(state, theta)\n",
    "\n",
    "            # Take action, observe reward and next state\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Compute TD error: δ = R + γ v̂(S', w) - v̂(S, w)\n",
    "            v_current = v_hat(state, w)\n",
    "            v_next = 0.0 if done else v_hat(next_state, w)  # v̂(S', w) = 0 if terminal\n",
    "            delta = reward + gamma * v_next - v_current\n",
    "\n",
    "            # Update eligibility traces\n",
    "            grad_v = x(state)  # ∇v̂(S, w) = x(s) for linear approximation\n",
    "            grad_pi = grad_log_pi(state, action, theta)  # ∇ln π(A|S, θ)\n",
    "            z_w = gamma * lambda_w * z_w + grad_v\n",
    "            z_theta = gamma * lambda_theta * z_theta + grad_pi\n",
    "\n",
    "            # Update critic: w ← w + α_w δ z_w\n",
    "            w += alpha_w * delta * z_w\n",
    "\n",
    "            # Update actor: θ ← θ + α_θ δ z_θ\n",
    "            theta += alpha_theta * delta * z_theta\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "    return theta, policy, w, v_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67758664-e328-48a3-8958-cddd039518d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "def actor_critic_traces_continuing(env, num_steps=10000, alpha_theta=0.01, alpha_w=0.1, alpha_R=0.01, gamma=0.9, lambda_w=0.8, lambda_theta=0.8):\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    d_policy = num_states * num_actions  # Feature dimension for policy (state-action pairs)\n",
    "    d_value = num_states  # Feature dimension for value function (states)\n",
    "\n",
    "    # Initialize policy and value function parameters\n",
    "    theta = np.zeros(d_policy)  # For π(a|s, θ)\n",
    "    w = np.zeros(d_value)  # For v(s, w)\n",
    "    R_bar = 0.0  # Average reward estimate\n",
    "\n",
    "    # Initialize eligibility traces\n",
    "    z_theta = np.zeros(d_policy)  # Eligibility trace for θ\n",
    "    z_w = np.zeros(d_value)  # Eligibility trace for w\n",
    "\n",
    "    # Feature function φ(s, a) for policy: one-hot encoding for state-action pairs\n",
    "    def phi(s, a):\n",
    "        features = np.zeros(d_policy)\n",
    "        features[s * num_actions + a] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Feature function x(s) for value function: one-hot encoding for states\n",
    "    def x(s):\n",
    "        features = np.zeros(d_value)\n",
    "        features[s] = 1.0\n",
    "        return features\n",
    "\n",
    "    # Softmax policy π(a|s, θ)\n",
    "    def policy(s, theta):\n",
    "        scores = np.array([np.dot(theta, phi(s, a)) for a in range(num_actions)])\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        probs = exp_scores / np.sum(exp_scores)\n",
    "        return probs\n",
    "\n",
    "    # Sample an action from the policy\n",
    "    def sample_action(s, theta):\n",
    "        probs = policy(s, theta)\n",
    "        return np.random.choice(num_actions, p=probs)\n",
    "\n",
    "    # Gradient of log π(a|s, θ)\n",
    "    def grad_log_pi(s, a, theta):\n",
    "        x_sa = phi(s, a)\n",
    "        probs = policy(s, theta)\n",
    "        expected_phi = np.zeros(d_policy)\n",
    "        for b in range(num_actions):\n",
    "            expected_phi += probs[b] * phi(s, b)\n",
    "        return x_sa - expected_phi\n",
    "\n",
    "    # State-value function v(s, w)\n",
    "    def v_hat(s, w):\n",
    "        return np.dot(w, x(s))\n",
    "\n",
    "    # Initialize state\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Main loop over time steps\n",
    "    for step in range(num_steps):\n",
    "        # Sample action\n",
    "        action = sample_action(state, theta)\n",
    "\n",
    "        # Take action, observe reward and next state\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Simulate continuing environment: if terminated, reset to start state\n",
    "        if terminated or truncated:\n",
    "            next_state, _ = env.reset()\n",
    "\n",
    "        # Compute TD error: δ = R - R̄ + v̂(S', w) - v̂(S, w)\n",
    "        v_current = v_hat(state, w)\n",
    "        v_next = v_hat(next_state, w)\n",
    "        delta = reward - R_bar + v_next - v_current\n",
    "\n",
    "        # Update average reward: R̄ ← R̄ + α_R δ\n",
    "        R_bar += alpha_R * delta\n",
    "\n",
    "        # Update eligibility traces\n",
    "        grad_v = x(state)  # ∇v̂(S, w) = x(s) for linear approximation\n",
    "        grad_pi = grad_log_pi(state, action, theta)  # ∇ln π(A|S, θ)\n",
    "        z_w = gamma * lambda_w * z_w + grad_v\n",
    "        z_theta = gamma * lambda_theta * z_theta + grad_pi\n",
    "\n",
    "        # Update critic: w ← w + α_w δ z_w\n",
    "        w += alpha_w * delta * z_w\n",
    "\n",
    "        # Update actor: θ ← θ + α_θ δ z_θ\n",
    "        theta += alpha_theta * delta * z_theta\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    return theta, policy, w, v_hat, R_bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc145f9b-6637-474d-bb36-01766f90414c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f878e-b7ac-4881-aa6f-35782ec67f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa295402-ff74-480a-b5d1-8d0dafe450b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d69338-c3f4-4900-81ad-6e8a0ad3cde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6f0b6-03cd-4f6f-88ad-da5b22aba804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedfc1bc-f958-4f94-acb3-e4f5153a3461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ae824-e5e5-42b3-815f-bb3fd4860aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
